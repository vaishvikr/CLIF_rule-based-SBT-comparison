{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import pyCLIF as pc\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tableone import TableOne, load_dataset\n",
    "con = pc.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv('../output/intermediate/study_cohort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort['sat_delivery_pass_fail'] = cohort['sat_delivery_pass_fail'].map({0:1,1:1})\n",
    "cohort['sat_screen_pass_fail'] = cohort['sat_screen_pass_fail'].map({0:1,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'event_time' is in datetime format\n",
    "cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "cohort['admission_dttm'] = pd.to_datetime(cohort['admission_dttm'], utc=True)\n",
    "cohort['discharge_dttm'] = pd.to_datetime(cohort['discharge_dttm'], utc=True)\n",
    "\n",
    "# Ensure the data is sorted by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "cohort['device_category_ffill'] = cohort.groupby('hospitalization_id')['device_category'].ffill()\n",
    "cohort['location_category_ffill'] = cohort.groupby('hospitalization_id')['location_category'].ffill()\n",
    "\n",
    "active_sedation_n_col = [\n",
    "    'fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine'\n",
    "]\n",
    "\n",
    "for col in active_sedation_n_col:\n",
    "    if col not in cohort.columns:\n",
    "        cohort[col] = np.nan\n",
    "        print(f\"Column '{col}' is missing. Please check your CLIF Meds table â€” it might be missing, or it's okay if your site doesn't use it.\")\n",
    "\n",
    "\n",
    "# Fill forward the meds by hospitalization columns by 'hosp_id'\n",
    "cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']] = cohort.groupby('hospitalization_id')[\n",
    "    ['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']\n",
    "].ffill()\n",
    "\n",
    "# Ensure the min value is greater than 0\n",
    "cohort['min_sedation_dose'] = cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam','hydromorphone','morphine']].min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_2'] = cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']].where(cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']] > 0).min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_non_ops'] = cohort[['propofol', 'lorazepam', 'midazolam']].min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_non_ops'] = cohort['min_sedation_dose_non_ops'].fillna(0)\n",
    "\n",
    "# Fill forward the paralytic by hospitalization columns by 'hosp_id'\n",
    "cohort[[\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"]] = cohort.groupby('hospitalization_id')[\n",
    "    [\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"]\n",
    "].ffill()\n",
    "# paralytic max to remove from consideration\n",
    "cohort['max_paralytics'] = cohort[[\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"\n",
    "        ]].max(axis=1, skipna=True).fillna(0)\n",
    "\n",
    "# Ensure the data is sorted again by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify eligible days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cohort(df):\n",
    "    df = df.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "    df['device_category_ffill'] = df.groupby('hospitalization_id')['device_category'].ffill()\n",
    "    df['location_category_ffill'] = df.groupby('hospitalization_id')['location_category'].ffill()\n",
    "    # Ensure 'event_time' is datetime\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "   \n",
    "    df['all_conditions_check'] = (\n",
    "            (df['device_category_ffill'].str.lower() == 'imv') &\n",
    "            (df['min_sedation_dose_2'] > 0) &\n",
    "            (df['location_category_ffill'].str.lower() == 'icu') &\n",
    "            (df['max_paralytics'] <= 0)\n",
    "        ).astype(int)\n",
    "\n",
    "    # Initialize result list\n",
    "    result = []\n",
    "\n",
    "    # Group by 'hospitalization_id' and 'date'\n",
    "    grouped_hosp = df.groupby(['hospitalization_id', df['event_time'].dt.normalize()])\n",
    "\n",
    "    # Use tqdm for the outer loop to show progress\n",
    "    for (hosp_id, date), group in tqdm(grouped_hosp, desc='Processing Hospitalizations by Date'):\n",
    "        group = group.sort_values('event_time')\n",
    "\n",
    "        # Get the entire hospitalization data for the current hospitalization_id\n",
    "        temp_df = df[df['hospitalization_id'] == hosp_id].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "        # Define start and end times for the current day\n",
    "        # Start time is 10 PM of the previous day\n",
    "        start_time = date - pd.Timedelta(days=1) + pd.Timedelta(hours=22)\n",
    "        # End time is 6 AM of the current day\n",
    "        end_time = date + pd.Timedelta(hours=6)\n",
    "\n",
    "        # Filter data in this time window for the entire hospitalization\n",
    "        mask_time = (temp_df['event_time'] >= start_time) & (temp_df['event_time'] <= end_time)\n",
    "        df_time_window = temp_df[mask_time].copy()\n",
    "\n",
    "        if df_time_window.empty:\n",
    "            continue\n",
    "\n",
    "        # Use the existing 'device_category_ffill' and 'location_category_ffill' columns\n",
    "        df_time_window['all_conditions_met'] = (df_time_window['all_conditions_check']>0\n",
    "        )\n",
    "\n",
    "        # If no times where all conditions are met, skip\n",
    "        if not df_time_window['all_conditions_met'].any():\n",
    "            continue\n",
    "\n",
    "        # Ensure data is sorted by 'event_time'\n",
    "        df_time_window = df_time_window.sort_values('event_time').reset_index(drop=True)\n",
    "\n",
    "        # Create a group identifier for continuous periods where conditions are met\n",
    "        df_time_window['condition_met_group'] = (df_time_window['all_conditions_met'] != df_time_window['all_conditions_met'].shift()).cumsum()\n",
    "\n",
    "        # Filter rows where all conditions are met\n",
    "        df_conditions = df_time_window[df_time_window['all_conditions_met']].copy()\n",
    "        if df_conditions.empty:\n",
    "            continue\n",
    "\n",
    "        # Group by 'condition_met_group' to identify continuous periods\n",
    "        grouped_conditions = df_conditions.groupby('condition_met_group')\n",
    "\n",
    "        found_four_hours = False\n",
    "        for group_id, group_df in grouped_conditions:\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "\n",
    "            # Calculate the duration of each continuous period where all conditions are met\n",
    "            group_df['duration'] = group_df['event_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "            group_df['cumulative_duration'] = group_df['duration'].cumsum()\n",
    "            total_duration = group_df['cumulative_duration'].iloc[-1]\n",
    "\n",
    "            if total_duration >= pd.Timedelta(hours=4):\n",
    "                # Calculate the exact event_time when cumulative duration reaches four hours\n",
    "                cumulative_duration = pd.Timedelta(seconds=0)\n",
    "                for idx in range(len(group_df)):\n",
    "                    cumulative_duration += group_df['duration'].iloc[idx]\n",
    "                    if cumulative_duration >= pd.Timedelta(hours=4):\n",
    "                        event_time_at_4_hours = group_df['event_time'].iloc[idx]\n",
    "                        break\n",
    "\n",
    "                # Append to result\n",
    "                result.append({\n",
    "                    'hospitalization_id': hosp_id,\n",
    "                    'current_day_key': date,\n",
    "                    'event_time_at_4_hours': event_time_at_4_hours\n",
    "                })\n",
    "                found_four_hours = True\n",
    "                # Since we found a period of at least 4 hours continuous conditions met, we can proceed to the next day\n",
    "                break  # Exit the loop over condition_met_group\n",
    "        if found_four_hours:\n",
    "            continue  # Proceed to the next day\n",
    "\n",
    "    # Convert result to DataFrame for better representation\n",
    "    result_df = pd.DataFrame(result)\n",
    "    return result_df\n",
    "\n",
    "result_df = process_cohort(cohort)\n",
    "print('Encounter days with at least 4 hours of conditions met from 10 PM to 6 AM:', len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result back into the cohort DataFrame\n",
    "cohort = cohort.merge(result_df[['hospitalization_id', 'current_day_key', 'event_time_at_4_hours']], \n",
    "                      how='left', \n",
    "                      left_on=['hospitalization_id', cohort['event_time'].dt.normalize()], \n",
    "                      right_on=['hospitalization_id', 'current_day_key'])\n",
    "\n",
    "# Initialize 'eligible_event' column with NaN and used for validation of exact time the event of 4 hr completed\n",
    "cohort['eligible_event'] = np.nan\n",
    "has_event_time = cohort['event_time_at_4_hours'].notna()\n",
    "for (hosp_id, date), group in cohort[has_event_time].groupby(['hospitalization_id', cohort['event_time'].dt.normalize()]):\n",
    "    event_time_at_4_hours = group['event_time_at_4_hours'].iloc[0]\n",
    "    subset = cohort[(cohort['hospitalization_id'] == hosp_id) & (cohort['event_time'] >= event_time_at_4_hours)]\n",
    "    if not subset.empty:\n",
    "        idx = subset['event_time'].idxmin()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "    else:\n",
    "        subset = cohort[cohort['hospitalization_id'] == hosp_id]\n",
    "        idx = subset['event_time'].idxmax()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "\n",
    "# fix where last row should not be eligible\n",
    "cohort = cohort.sort_values(['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "for hosp_id, group in cohort.groupby('hospitalization_id'):\n",
    "    last_idx = group.index[-1]\n",
    "    if cohort.loc[last_idx, 'eligible_event'] == 1:\n",
    "        cohort.loc[last_idx, 'eligible_event'] = np.nan\n",
    "\n",
    "\n",
    "# Flag all that date rows where eligible_event = 1\n",
    "filtered_cohort = cohort[cohort['eligible_event'] == 1][['hosp_id_day_key', 'eligible_event']]\n",
    "merged_cohort = cohort.merge(filtered_cohort, on='hosp_id_day_key', how='left', suffixes=('', '_filtered'))\n",
    "merged_cohort['on_vent_and_sedation'] = merged_cohort['eligible_event_filtered'].fillna(0).astype(int)\n",
    "merged_cohort = merged_cohort.drop(columns=['eligible_event_filtered'])\n",
    "\n",
    "del filtered_cohort,result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cohort['eligible_event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cohort[merged_cohort['on_vent_and_sedation']==1]['hosp_id_day_key'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_cohort[merged_cohort['on_vent_and_sedation']==1].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)  \n",
    "\n",
    "df['rank_sedation'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys'):\n",
    "    zero_mask = hosp_data['min_sedation_dose'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation'] = ranks.replace(0, np.nan)\n",
    "\n",
    "\n",
    "df['rank_sedation_non_ops'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys'):\n",
    "    zero_mask = hosp_data['min_sedation_dose_non_ops'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation_non_ops'] = ranks.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAT EHR all meds hard stop flaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SAT_EHR_delivery'] = np.nan\n",
    "med_columns = ['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']\n",
    "\n",
    "# Use groupby and vectorized operations for meds check\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys for meds check'):\n",
    "    hosp_data_sorted = hosp_data.sort_values('event_time')\n",
    "    for index, row in hosp_data_sorted.iterrows():\n",
    "        if not np.isnan(row['rank_sedation']):\n",
    "            current_time = row['event_time']\n",
    "            thirty_min_forward = hosp_data_sorted[(hosp_data_sorted['event_time'] >= current_time) &\n",
    "                                                  (hosp_data_sorted['event_time'] <= current_time + pd.Timedelta(minutes=30))]\n",
    "            # Check if all med_columns are either NaN or 0 and device & location categories are \"imv\" and \"icu\" in this timeframe\n",
    "            if (\n",
    "                 (thirty_min_forward[med_columns].isna() | (thirty_min_forward[med_columns] == 0)).all(axis=None) and\n",
    "                 (thirty_min_forward['device_category_ffill'] == 'imv').all() and\n",
    "                 (thirty_min_forward['location_category_ffill'] == 'icu').all()\n",
    "            ):\n",
    "                df.at[index, 'SAT_EHR_delivery'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAT EHR all meds hard stop flaging (modified meds / non ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SAT_modified_delivery'] = np.nan\n",
    "med_columns = ['propofol', 'lorazepam', 'midazolam']\n",
    "\n",
    "# Use groupby and vectorized operations for meds check\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys for meds check'):\n",
    "    hosp_data_sorted = hosp_data.sort_values('event_time')\n",
    "    for index, row in hosp_data_sorted.iterrows():\n",
    "        if not np.isnan(row['rank_sedation_non_ops']):\n",
    "            current_time = row['event_time']\n",
    "            thirty_min_forward = hosp_data_sorted[(hosp_data_sorted['event_time'] >= current_time) &\n",
    "                                                  (hosp_data_sorted['event_time'] <= current_time + pd.Timedelta(minutes=30))]\n",
    "\n",
    "            # Check if all med_columns are either NaN or 0 and device & location categories are \"imv\" and \"icu\" in this timeframe\n",
    "            if (\n",
    "                (thirty_min_forward[med_columns].isna() | (thirty_min_forward[med_columns] == 0)).all(axis=None) and\n",
    "                 (thirty_min_forward['device_category_ffill'] == 'imv').all() and\n",
    "                 (thirty_min_forward['location_category_ffill'] == 'icu').all()\n",
    "            ):\n",
    "                df.at[index, 'SAT_modified_delivery'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Icu los calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_los = cohort[['hospitalization_id', 'event_time', 'location_category_ffill']]\n",
    "icu_los = icu_los.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "icu_los['segment'] = (icu_los['location_category_ffill'] != icu_los['location_category_ffill'].shift()).cumsum()\n",
    "\n",
    "icu_segments = icu_los[icu_los['location_category_ffill'].str.lower() == 'icu'].groupby(\n",
    "    ['hospitalization_id', 'segment']\n",
    ").agg(\n",
    "    location_start=('event_time', 'first'),\n",
    "    location_end=('event_time', 'last')\n",
    ").reset_index()\n",
    "\n",
    "icu_segments['los_days'] = (icu_segments['location_end'] - icu_segments['location_start']).dt.total_seconds() / (24 * 3600)\n",
    "icu_los_per_encounter = icu_segments[['hospitalization_id', 'los_days']]\n",
    "\n",
    "total_icu_los_per_hosp = icu_los_per_encounter.groupby('hospitalization_id', as_index=False).agg(\n",
    "    ICU_LOS=('los_days', 'sum')\n",
    ")\n",
    "total_icu_los_per_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last dishcharge hosptial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hosp = cohort[['hospitalization_id', 'event_time', 'hospital_id']]\n",
    "\n",
    "last_hosp = last_hosp.sort_values(by=['hospitalization_id','event_time'], ascending=False).groupby(\n",
    "    ['hospitalization_id'], as_index=False\n",
    ").agg(({'hospital_id': 'first'})).reset_index(drop=True)\n",
    "last_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table one df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = df[['patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "       'age_at_admission', 'discharge_category', 'sex_category',\n",
    "       'race_category', 'ethnicity_category','hosp_id_day_key']].drop_duplicates()\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.merge(main, total_icu_los_per_hosp, on='hospitalization_id', how='left')\n",
    "main = pd.merge(main, last_hosp, on='hospitalization_id', how='left')\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to group by\n",
    "group_cols = [\n",
    " 'hosp_id_day_key'\n",
    "]\n",
    "\n",
    "max_cols = ['sat_screen_pass_fail','sat_delivery_pass_fail','SAT_EHR_delivery', 'SAT_modified_delivery', 'eligible_event']\n",
    "agg_dict = {col: 'max' for col in max_cols}\n",
    "\n",
    "df_grouped = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "df_grouped = df_grouped.sort_values('hosp_id_day_key').reset_index(drop=True)\n",
    "\n",
    "df_grouped['sat_flowsheet_delivery_flag'] = np.where(\n",
    "    (\n",
    "        (df_grouped['sat_screen_pass_fail'] == 1) |\n",
    "        (df_grouped['sat_delivery_pass_fail'] == 1)\n",
    "    ) &\n",
    "    (df_grouped['eligible_event'] == 1),\n",
    "    1,  # Flag is set to 1 (True) if conditions are met\n",
    "    np.nan   # Flag nan\n",
    ")\n",
    "\n",
    "final_df = main.merge(df_grouped, on='hosp_id_day_key', how='inner')\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['sat_delivery_pass_fail', 'sat_screen_pass_fail', 'SAT_EHR_delivery',\n",
    "       'SAT_modified_delivery', 'eligible_event',\n",
    "       'sat_flowsheet_delivery_flag']:\n",
    "    print(final_df[x].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../output/intermediate/final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table one print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex_category', 'race_category', 'ethnicity_category','discharge_category']\n",
    "non_categorical_columns = ['age_at_admission',  'ICU_LOS', 'Inpatient_LOS']\n",
    "\n",
    "final_df['admission_dttm'] = pd.to_datetime(final_df['admission_dttm'],utc=True)\n",
    "final_df['discharge_dttm'] = pd.to_datetime(final_df['discharge_dttm'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAT FLAG Table 1\n",
    "\n",
    "\n",
    "sat_flow_t1 = final_df[final_df['sat_flowsheet_delivery_flag'] == 1][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_flow_t1['Inpatient_LOS'] = (sat_flow_t1['discharge_dttm'] - sat_flow_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_flow_t1)>1:\n",
    "    table1 = TableOne(sat_flow_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table1.to_csv(f'../output/final/table1_sat_flowhseet_{pc.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "    print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAT EHR FLAG Table 1\n",
    "\n",
    "sat_ehr_t1 = final_df[(final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_ehr_t1['Inpatient_LOS'] = (sat_ehr_t1['discharge_dttm'] - sat_ehr_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_ehr_t1)>1:\n",
    "    table2 = TableOne(sat_ehr_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table2.to_csv(f'../output/final/table1_sat_ehr_{pc.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "    print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all Table 1\n",
    "\n",
    "all_t1 = final_df[[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "all_t1['Inpatient_LOS'] = (all_t1['discharge_dttm'] - all_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(all_t1)>1:\n",
    "    table3 = TableOne(all_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table3.to_csv(f'../output/final/table1_all_t1_{pc.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "    print(table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per hospital stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store each hospital's data\n",
    "data_list = []\n",
    "\n",
    "# Iterate over unique hospital IDs as strings\n",
    "for x in final_df['hospital_id'].astype(str).unique():\n",
    "    # Calculate counts based on specific conditions\n",
    "    eligible_event_count = final_df[(final_df['eligible_event'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    sat_flowsheet_delivery_flag_count = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    SAT_modified_delivery_count = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    SAT_EHR_delivery_count = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "\n",
    "    SAT_EHR_uni_pats = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_EHR_hosp = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_modified_uni_pats = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_modified_hosp = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_EHR_modified_uni_pats = final_df[((final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_EHR_modified_hosp = final_df[((final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_flowsheet_uni_pats = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_flowsheet_hosp = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    # Safeguard against division by zero\n",
    "    if eligible_event_count > 0:\n",
    "        percent_sat_flowsheet_delivery_flag = (sat_flowsheet_delivery_flag_count / eligible_event_count) * 100\n",
    "        percent_SAT_modified_delivery = (SAT_modified_delivery_count / eligible_event_count) * 100\n",
    "        percent_SAT_EHR_delivery = (SAT_EHR_delivery_count / eligible_event_count) * 100\n",
    "    else:\n",
    "        percent_sat_flowsheet_delivery_flag = 0\n",
    "        percent_SAT_modified_delivery = 0\n",
    "        percent_SAT_EHR_delivery = 0\n",
    "\n",
    "    # Append the data for this hospital to the list\n",
    "    data_list.append({\n",
    "        'Site_Name_Hosp': pc.helper[\"site_name\"] + '_' + x,  \n",
    "        '%_of_SAT_flowsheet_delivery_flag': percent_sat_flowsheet_delivery_flag,\n",
    "        '%_of_SAT_modified_delivery': percent_SAT_modified_delivery,\n",
    "        '%_of_SAT_EHR_delivery': percent_SAT_EHR_delivery,\n",
    "        'eligible_event_count': eligible_event_count,\n",
    "        'sat_flowsheet_delivery_flag_count': sat_flowsheet_delivery_flag_count,\n",
    "        'SAT_modified_delivery_count': SAT_modified_delivery_count,\n",
    "        'SAT_EHR_delivery_count': SAT_EHR_delivery_count,\n",
    "\n",
    "        'SAT_EHR_unique_patients': SAT_EHR_uni_pats,\n",
    "        'SAT_EHR_unique_hospitalizations': SAT_EHR_hosp,\n",
    "        'SAT_modified_unique_patients': SAT_modified_uni_pats,\n",
    "        'SAT_modified_unique_hospitalizations': SAT_modified_hosp,\n",
    "        'SAT_EHR_modified_unique_patients': SAT_EHR_modified_uni_pats,\n",
    "        'SAT_EHR_modified_unique_hospitalizations': SAT_EHR_modified_hosp,\n",
    "        'SAT_flowsheet_unique_patients': SAT_flowsheet_uni_pats, \n",
    "        'SAT_flowsheet_unique_hospitalizations': SAT_flowsheet_hosp   \n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "final_data_df = pd.DataFrame(data_list)\n",
    "final_data_df.to_csv(f'../output/final/sat_stats_{pc.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "# Display the final DataFrame\n",
    "final_data_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank You!!! keep latest timestamp files and upload to box :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
