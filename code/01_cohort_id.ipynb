{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n",
      "{'site_name': 'RUSH', 'tables_path': 'C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/', 'file_type': 'csv'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import pyCLIF as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_adt.csv\n",
      "Count with hours and minutes: 1072486\n",
      "Count without hours and minutes: 0\n"
     ]
    }
   ],
   "source": [
    "adt = pc.load_data('clif_adt')\n",
    "adt= adt[['hospitalization_id','in_dttm','location_category','hospital_id']]\n",
    "adt['in_dttm'] = pc.getdttm(adt['in_dttm'])\n",
    "pc.deftime(adt['in_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cohort filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e4c8fb93784c1fad0eafd7acdeb5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_respiratory_support.csv\n",
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_hospitalization.csv\n",
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_patient.csv\n",
      "3553  : potential cohort count\n"
     ]
    }
   ],
   "source": [
    "rst_col = [ 'hospitalization_id', 'recorded_dttm', 'device_category', 'mode_category','fio2_set']\n",
    "rst = pc.load_data('clif_respiratory_support')\n",
    "rst = rst[rst_col]\n",
    "\n",
    "hosp = pc.load_data('clif_hospitalization')\n",
    "pat = pc.load_data('clif_patient')\n",
    "\n",
    "imv_hosp_ids = rst[rst['device_category'].str.lower()=='imv'].hospitalization_id.unique()\n",
    "icu_hosp_ids = adt[adt['location_category'].str.lower()=='icu'].hospitalization_id.unique()\n",
    "\n",
    "icu_hosp_ids = [x for x in icu_hosp_ids if x is not None]\n",
    "imv_hosp_ids = [x for x in imv_hosp_ids if x is not None]\n",
    "\n",
    "hosp = hosp[\n",
    "    (hosp['admission_dttm'].dt.year >= 2020) &\n",
    "    (hosp['admission_dttm'].dt.year <= 2021) &\n",
    "    (hosp['hospitalization_id'].isin(np.intersect1d(imv_hosp_ids, icu_hosp_ids))) &\n",
    "    (hosp['age_at_admission'] <=119)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "required_id= hosp['hospitalization_id'].unique()\n",
    "print(len(required_id),' : potential cohort count')\n",
    "\n",
    "base = pd.merge(hosp,pat,on='patient_id',how='inner')\\\n",
    "[['patient_id', 'hospitalization_id','admission_dttm', 'discharge_dttm','age_at_admission', 'discharge_category','sex_category','race_category', 'ethnicity_category']]\n",
    "\n",
    "base['admission_dttm'] = pc.getdttm(base['admission_dttm'])\n",
    "\n",
    "base.columns\n",
    "\n",
    "adt = adt[adt['hospitalization_id'].isin(required_id)].reset_index(drop=True)\n",
    "rst = rst[rst['hospitalization_id'].isin(required_id)].reset_index(drop=True)\n",
    "\n",
    "del hosp,pat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resp Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = rst[rst['hospitalization_id'].isin(required_id)].reset_index(drop=True)\n",
    "rst['device_category'] = rst['device_category'].str.lower()\n",
    "rst['recorded_dttm_sec'] = pc.getdttm(rst['recorded_dttm'],cutby=None)\n",
    "rst['recorded_dttm'] = pc.getdttm(rst['recorded_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = rst.sort_values(by=['hospitalization_id','recorded_dttm_sec'], ascending=False).groupby(\n",
    "    ['hospitalization_id', 'recorded_dttm'], as_index=False\n",
    ").agg({'device_category': 'first', 'mode_category': 'first','fio2_set':'first'}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with hours and minutes: 1655141\n",
      "Count without hours and minutes: 0\n"
     ]
    }
   ],
   "source": [
    "pc.deftime(rst['recorded_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_medication_admin_continuous.csv\n"
     ]
    }
   ],
   "source": [
    "mac = pc.load_data('clif_medication_admin_continuous')\n",
    "mac_col = ['hospitalization_id', 'admin_dttm','med_dose','med_category']\n",
    "mac = mac[(mac['hospitalization_id'].isin(required_id)) & (mac['med_category'].isin( [\n",
    "        \"norepinephrine\",\n",
    "        \"epinephrine\",\n",
    "        \"phenylephrine\",\n",
    "        \"angiotensin\",\n",
    "        \"vasopressin\",\n",
    "        \"dopamine\",\n",
    "        \"dobutamine\",\n",
    "        \"milrinone\",\n",
    "        \"isoproterenol\",\n",
    "    ]))][mac_col].reset_index(drop=True)\n",
    "mac['admin_dttm'] = pc.getdttm(mac['admin_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a4ae9b42ed450eaea403d7f581eb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from C:/Users/vchaudha/OneDrive - rush.edu/ATS2024/RUSH_CLIF/clif_patient_assessments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vchaudha\\AppData\\Local\\Temp\\ipykernel_32344\\3157011915.py:21: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  pat_at['assessment_value'] = pat_at['numerical_value'].combine_first(pat_at['categorical_value'])\n"
     ]
    }
   ],
   "source": [
    "cat_values_mapping_dict = {\n",
    "    'negative': 1,\n",
    "    'fail': 1,\n",
    "    'pass': 1,\n",
    "    'positive': 1,\n",
    "    None: np.nan ,\n",
    "    np.nan : np.nan,\n",
    "    'yes':1,\n",
    "    'no':1\n",
    "}\n",
    "\n",
    "pat_assess_cats_rquired = [ 'sbt_delivery_pass_fail',\n",
    "                            'sbt_screen_pass_fail']\n",
    "\n",
    "pat_at = pc.load_data('clif_patient_assessments',-1)\n",
    "pat_at_col = ['hospitalization_id', 'recorded_dttm','numerical_value', 'categorical_value','assessment_category']\n",
    "pat_at['assessment_category'] = pat_at['assessment_category'].str.lower()\n",
    "pat_at = pat_at[(pat_at['hospitalization_id'].isin(required_id)) & (pat_at['assessment_category'].isin(pat_assess_cats_rquired)) ][pat_at_col].reset_index(drop=True)\n",
    "pat_at['recorded_dttm'] = pc.getdttm(pat_at['recorded_dttm'])\n",
    "pat_at['categorical_value'] = pat_at['categorical_value'].str.lower().map(cat_values_mapping_dict)\n",
    "pat_at['assessment_value'] = pat_at['numerical_value'].combine_first(pat_at['categorical_value'])\n",
    "pat_at.drop(columns=['numerical_value','categorical_value'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_at['assessment_value'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0941df661dc4a578b6429bb68a096ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vit = pc.load_data('clif_vitals',-1)\n",
    "vit_col = ['hospitalization_id','recorded_dttm','vital_category','vital_value' ]\n",
    "vit = vit[vit_col]\n",
    "vit['vital_category'] = vit['vital_category'].str.lower()\n",
    "vit = vit[vit['vital_category'].isin(['map','heart_rate','sbp','dbp','spo2','respiratory_rate'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.register(\"base\", base)\n",
    "duckdb.register(\"pat_at\", pat_at)\n",
    "duckdb.register(\"rst\", rst)\n",
    "duckdb.register(\"mac\", mac)\n",
    "duckdb.register('adt',adt)\n",
    "\n",
    "q=\"\"\"\n",
    "WITH\n",
    "    uni_event_dttm as (\n",
    "        select distinct\n",
    "            hospitalization_id,\n",
    "            event_time\n",
    "        from\n",
    "            (\n",
    "                SELECT\n",
    "                    hospitalization_id,\n",
    "                    in_dttm AS event_time\n",
    "                FROM\n",
    "                    adt\n",
    "                where\n",
    "                    in_dttm is not null\n",
    "                UNION\n",
    "                SELECT\n",
    "                    hospitalization_id,\n",
    "                    recorded_dttm AS event_time\n",
    "                FROM\n",
    "                    rst\n",
    "                where\n",
    "                    recorded_dttm is not null\n",
    "                UNION\n",
    "                SELECT\n",
    "                    hospitalization_id,\n",
    "                    recorded_dttm AS event_time\n",
    "                FROM\n",
    "                    pat_at\n",
    "                where\n",
    "                    recorded_dttm is not null\n",
    "                UNION\n",
    "                SELECT\n",
    "                    hospitalization_id,\n",
    "                    admin_dttm AS event_time\n",
    "                FROM\n",
    "                    mac\n",
    "                where\n",
    "                    admin_dttm is not null\n",
    "            ) uni_time\n",
    "    )\n",
    "select distinct\n",
    "    patient_id,\n",
    "    a.hospitalization_id,\n",
    "    admission_dttm,\n",
    "    discharge_dttm,\n",
    "    age_at_admission,\n",
    "    discharge_category,\n",
    "    sex_category,\n",
    "    race_category,\n",
    "    ethnicity_category,\n",
    "    event_time\n",
    "from\n",
    "    base a\n",
    "    left join uni_event_dttm b on a.hospitalization_id = b.hospitalization_id\n",
    "\"\"\"\n",
    "wide_cohort_df = duckdb.sql(q).df()\n",
    "pc.deftime(wide_cohort_df['event_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pivots for assessment and mac table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH pas_data AS (\n",
    "    SELECT  distinct assessment_value ,\tassessment_category\t,\n",
    "    hospitalization_id || '_' || strftime(recorded_dttm, '%Y%m%d%H%M') AS combo_id\n",
    "    FROM pat_at where recorded_dttm is not null \n",
    ") \n",
    "PIVOT pas_data\n",
    "ON assessment_category\n",
    "USING first(assessment_value)\n",
    "GROUP BY combo_id\n",
    "\"\"\"\n",
    "p_pas = duckdb.sql(query).df()\n",
    "\n",
    "query = \"\"\"\n",
    "WITH mac_data AS (\n",
    "    SELECT  distinct med_dose ,\tmed_category\t,\n",
    "    hospitalization_id || '_' || strftime(admin_dttm, '%Y%m%d%H%M') AS combo_id\n",
    "    FROM mac where admin_dttm is not null \n",
    ") \n",
    "PIVOT mac_data\n",
    "ON med_category\n",
    "USING min(med_dose)\n",
    "GROUP BY combo_id\n",
    "\"\"\"\n",
    "p_mac = duckdb.sql(query).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### id-ing all unique timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.register(\"expanded_df\", wide_cohort_df)\n",
    "duckdb.register(\"p_pas\", p_pas)\n",
    "duckdb.register(\"p_mac\", p_mac)\n",
    "\n",
    "q=\"\"\"\n",
    "  WITH\n",
    "    u_rst as (\n",
    "        select\n",
    "            *,\n",
    "            hospitalization_id || '_' || strftime (recorded_dttm, '%Y%m%d%H%M') AS combo_id\n",
    "        from\n",
    "            rst\n",
    "    ),\n",
    "    u_adt as (\n",
    "        select\n",
    "            *,\n",
    "            hospitalization_id || '_' || strftime (in_dttm, '%Y%m%d%H%M') AS combo_id\n",
    "        from\n",
    "            adt\n",
    "    ),\n",
    "    u_expanded_df as (\n",
    "        select\n",
    "            *,\n",
    "            hospitalization_id || '_' || strftime (event_time, '%Y%m%d%H%M') AS combo_id\n",
    "        from\n",
    "            expanded_df\n",
    "    )\n",
    "select\n",
    "    *\n",
    "from\n",
    "    u_expanded_df a\n",
    "    left join u_adt d on a.combo_id = d.combo_id\n",
    "    left join u_rst e on a.combo_id = e.combo_id\n",
    "    left join p_mac g on a.combo_id = g.combo_id\n",
    "    left join p_pas h on a.combo_id = h.combo_id\n",
    "\n",
    "                    \n",
    "\"\"\"\n",
    "\n",
    "all_join_df = duckdb.sql(q).df().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_join_df.shape[0] != wide_cohort_df.shape[0]:\n",
    "    print('Data has duplicates or same timestamp issue, contact project owner')\n",
    "else:\n",
    "    del rst,mac,pat_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removing wide-supporting columns and adding forward fills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_df.drop(columns= ['hospitalization_id_1','hospitalization_id_2','combo_id','combo_id_1', 'combo_id_2' ,'combo_id_3','recorded_dttm','combo_id_4','in_dttm'], axis = 1,inplace=True)\n",
    "\n",
    "all_join_df['event_time'] = pd.to_datetime(all_join_df['event_time'])\n",
    "all_join_df['date'] = all_join_df['event_time'].dt.date\n",
    "\n",
    "all_join_df = all_join_df.sort_values(['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "all_join_df['device_category_ffill'] = all_join_df.groupby('hospitalization_id')['device_category'].ffill()\n",
    "all_join_df['location_category_ffill'] = all_join_df.groupby('hospitalization_id')['location_category'].ffill()\n",
    "# Assign day numbers to each 'hospitalization_id'\n",
    "all_join_df['day_number'] = all_join_df.groupby('hospitalization_id')['date'].rank(method='dense').astype(int)\n",
    "\n",
    "# Create the combo_key by combining 'hospitalization_id' and 'day_number'\n",
    "all_join_df['hosp_id_day_key'] = all_join_df['hospitalization_id'].astype(str) + '_day_' + all_join_df['day_number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['sat_delivery_pass_fail', 'sat_screen_pass_fail']\n",
    "for col in columns_to_check:\n",
    "    if col not in all_join_df.columns:\n",
    "        all_join_df[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_df.to_csv('../output/intermediate/study_cohort.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".satsbt_ATS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
