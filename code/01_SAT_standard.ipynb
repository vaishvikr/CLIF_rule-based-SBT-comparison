{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n",
      "{'site_name': 'RUSH', 'tables_path': 'C:/Users/vchaudha/Downloads/rush_parquet/', 'file_type': 'parquet', 'your_site_timezone': 'US/Central'}\n",
      "Imported SBT Helper!\n",
      "Loaded configuration from config.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "import pyCLIF as pc\n",
    "import pySBT as t1code\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tableone import TableOne, load_dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "con = pc.load_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv('../output/intermediate/study_cohort.csv')\n",
    "t1_cohort = cohort.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../output/final/RUSH\\SAT_standard' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Construct the full directory path\n",
    "directory_path = os.path.join(\"../output/final/\", pc.helper[\"site_name\"], \"SAT_standard\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort['sat_delivery_pass_fail'] = cohort['sat_delivery_pass_fail'].map({0:1,1:1})\n",
    "cohort['sat_screen_pass_fail'] = cohort['sat_screen_pass_fail'].map({0:1,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'event_time' is in datetime format\n",
    "cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "cohort['admission_dttm'] = pd.to_datetime(cohort['admission_dttm'], utc=True)\n",
    "cohort['discharge_dttm'] = pd.to_datetime(cohort['discharge_dttm'], utc=True)\n",
    "\n",
    "# Ensure the data is sorted by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "cohort['device_category_ffill'] = cohort.groupby('hospitalization_id')['device_category'].ffill()\n",
    "cohort['location_category_ffill'] = cohort.groupby('hospitalization_id')['location_category'].ffill()\n",
    "\n",
    "active_sedation_n_col = [\n",
    "    'fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine'\n",
    "]\n",
    "\n",
    "for col in active_sedation_n_col:\n",
    "    if col not in cohort.columns:\n",
    "        cohort[col] = np.nan\n",
    "        print(f\"Column '{col}' is missing. Please check your CLIF Meds table — it might be missing, or it's okay if your site doesn't use it.\")\n",
    "\n",
    "\n",
    "# Fill forward the meds by hospitalization columns by 'hosp_id'\n",
    "cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']] = cohort.groupby('hospitalization_id')[\n",
    "    ['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']\n",
    "].ffill()\n",
    "\n",
    "# Ensure the min value is greater than 0\n",
    "cohort['min_sedation_dose'] = cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam','hydromorphone','morphine']].min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_2'] = cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']].where(cohort[['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']] > 0).min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_non_ops'] = cohort[['propofol', 'lorazepam', 'midazolam']].min(axis=1, skipna=True)\n",
    "cohort['min_sedation_dose_non_ops'] = cohort['min_sedation_dose_non_ops'].fillna(0)\n",
    "\n",
    "# Fill forward the paralytic by hospitalization columns by 'hosp_id'\n",
    "cohort[[\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"]] = cohort.groupby('hospitalization_id')[\n",
    "    [\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"]\n",
    "].ffill()\n",
    "# paralytic max to remove from consideration\n",
    "cohort['max_paralytics'] = cohort[[\"cisatracurium\"\n",
    "        ,\"vecuronium\"\n",
    "        ,\"rocuronium\"\n",
    "        ]].max(axis=1, skipna=True).fillna(0)\n",
    "\n",
    "# Ensure the data is sorted again by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify eligible days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Hospitalizations by Date: 100%|██████████| 18387/18387 [02:45<00:00, 111.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter days with at least 4 hours of conditions met from 10 PM to 6 AM: 7955\n"
     ]
    }
   ],
   "source": [
    "def process_cohort(df):\n",
    "    df = df.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "    df['device_category_ffill'] = df.groupby('hospitalization_id')['device_category'].ffill()\n",
    "    df['location_category_ffill'] = df.groupby('hospitalization_id')['location_category'].ffill()\n",
    "    # Ensure 'event_time' is datetime\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "   \n",
    "    df['all_conditions_check'] = (\n",
    "            (df['device_category_ffill'].str.lower() == 'imv') &\n",
    "            (df['min_sedation_dose_2'] > 0) &\n",
    "            (df['location_category_ffill'].str.lower() == 'icu') &\n",
    "            (df['max_paralytics'] <= 0)\n",
    "        ).astype(int)\n",
    "\n",
    "    # Initialize result list\n",
    "    result = []\n",
    "\n",
    "    vented_day = df[(df['device_category'] == 'imv')]['hosp_id_day_key'].unique()\n",
    "    # Group by 'hospitalization_id' and 'date'\n",
    "    grouped_hosp = df[df['hosp_id_day_key'].isin(vented_day)].groupby(['hospitalization_id', df['event_time'].dt.normalize()])\n",
    "    \n",
    "    # Use tqdm for the outer loop to show progress\n",
    "    for (hosp_id, date), group in tqdm(grouped_hosp, desc='Processing Hospitalizations by Date'):\n",
    "        group = group.sort_values('event_time')\n",
    "\n",
    "        # Get the entire hospitalization data for the current hospitalization_id\n",
    "        temp_df = df[df['hospitalization_id'] == hosp_id].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "        # Define start and end times for the current day\n",
    "        # Start time is 10 PM of the previous day\n",
    "        start_time = date - pd.Timedelta(days=1) + pd.Timedelta(hours=22)\n",
    "        # End time is 6 AM of the current day\n",
    "        end_time = date + pd.Timedelta(hours=6)\n",
    "\n",
    "        # Filter data in this time window for the entire hospitalization\n",
    "        mask_time = (temp_df['event_time'] >= start_time) & (temp_df['event_time'] <= end_time)\n",
    "        df_time_window = temp_df[mask_time].copy()\n",
    "\n",
    "        if df_time_window.empty:\n",
    "            continue\n",
    "\n",
    "        # Use the existing 'device_category_ffill' and 'location_category_ffill' columns\n",
    "        df_time_window['all_conditions_met'] = (df_time_window['all_conditions_check']>0\n",
    "        )\n",
    "\n",
    "        # If no times where all conditions are met, skip\n",
    "        if not df_time_window['all_conditions_met'].any():\n",
    "            continue\n",
    "\n",
    "        # Ensure data is sorted by 'event_time'\n",
    "        df_time_window = df_time_window.sort_values('event_time').reset_index(drop=True)\n",
    "\n",
    "        # Create a group identifier for continuous periods where conditions are met\n",
    "        df_time_window['condition_met_group'] = (df_time_window['all_conditions_met'] != df_time_window['all_conditions_met'].shift()).cumsum()\n",
    "\n",
    "        # Filter rows where all conditions are met\n",
    "        df_conditions = df_time_window[df_time_window['all_conditions_met']].copy()\n",
    "        if df_conditions.empty:\n",
    "            continue\n",
    "\n",
    "        # Group by 'condition_met_group' to identify continuous periods\n",
    "        grouped_conditions = df_conditions.groupby('condition_met_group')\n",
    "\n",
    "        found_four_hours = False\n",
    "        for group_id, group_df in grouped_conditions:\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "\n",
    "            # Calculate the duration of each continuous period where all conditions are met\n",
    "            group_df['duration'] = group_df['event_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "            group_df['cumulative_duration'] = group_df['duration'].cumsum()\n",
    "            total_duration = group_df['cumulative_duration'].iloc[-1]\n",
    "\n",
    "            if total_duration >= pd.Timedelta(hours=4):\n",
    "                # Calculate the exact event_time when cumulative duration reaches four hours\n",
    "                cumulative_duration = pd.Timedelta(seconds=0)\n",
    "                for idx in range(len(group_df)):\n",
    "                    cumulative_duration += group_df['duration'].iloc[idx]\n",
    "                    if cumulative_duration >= pd.Timedelta(hours=4):\n",
    "                        event_time_at_4_hours = group_df['event_time'].iloc[idx]\n",
    "                        break\n",
    "\n",
    "                # Append to result\n",
    "                result.append({\n",
    "                    'hospitalization_id': hosp_id,\n",
    "                    'current_day_key': date,\n",
    "                    'event_time_at_4_hours': event_time_at_4_hours\n",
    "                })\n",
    "                found_four_hours = True\n",
    "                # Since we found a period of at least 4 hours continuous conditions met, we can proceed to the next day\n",
    "                break  # Exit the loop over condition_met_group\n",
    "        if found_four_hours:\n",
    "            continue  # Proceed to the next day\n",
    "\n",
    "    # Convert result to DataFrame for better representation\n",
    "    result_df = pd.DataFrame(result)\n",
    "    return result_df\n",
    "\n",
    "result_df = process_cohort(cohort)\n",
    "print('Encounter days with at least 4 hours of conditions met from 10 PM to 6 AM:', len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result back into the cohort DataFrame\n",
    "cohort = cohort.merge(result_df[['hospitalization_id', 'current_day_key', 'event_time_at_4_hours']], \n",
    "                      how='left', \n",
    "                      left_on=['hospitalization_id', cohort['event_time'].dt.normalize()], \n",
    "                      right_on=['hospitalization_id', 'current_day_key'])\n",
    "\n",
    "# Initialize 'eligible_event' column with NaN and used for validation of exact time the event of 4 hr completed\n",
    "cohort['eligible_event'] = np.nan\n",
    "has_event_time = cohort['event_time_at_4_hours'].notna()\n",
    "for (hosp_id, date), group in cohort[has_event_time].groupby(['hospitalization_id', cohort['event_time'].dt.normalize()]):\n",
    "    event_time_at_4_hours = group['event_time_at_4_hours'].iloc[0]\n",
    "    subset = cohort[(cohort['hospitalization_id'] == hosp_id) & (cohort['event_time'] >= event_time_at_4_hours)]\n",
    "    if not subset.empty:\n",
    "        idx = subset['event_time'].idxmin()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "    else:\n",
    "        subset = cohort[cohort['hospitalization_id'] == hosp_id]\n",
    "        idx = subset['event_time'].idxmax()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "\n",
    "# fix where last row should not be eligible\n",
    "cohort = cohort.sort_values(['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "for hosp_id, group in cohort.groupby('hospitalization_id'):\n",
    "    last_idx = group.index[-1]\n",
    "    if cohort.loc[last_idx, 'eligible_event'] == 1:\n",
    "        cohort.loc[last_idx, 'eligible_event'] = np.nan\n",
    "\n",
    "\n",
    "# Flag all that date rows where eligible_event = 1\n",
    "filtered_cohort = cohort[cohort['eligible_event'] == 1][['hosp_id_day_key', 'eligible_event']]\n",
    "merged_cohort = cohort.merge(filtered_cohort, on='hosp_id_day_key', how='left', suffixes=('', '_filtered'))\n",
    "merged_cohort['on_vent_and_sedation'] = merged_cohort['eligible_event_filtered'].fillna(0).astype(int)\n",
    "merged_cohort = merged_cohort.drop(columns=['eligible_event_filtered'])\n",
    "\n",
    "del filtered_cohort,result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eligible_event\n",
       "1.0    7943\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_cohort['eligible_event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7943"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_cohort[merged_cohort['on_vent_and_sedation']==1]['hosp_id_day_key'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hosp_id_day_keys: 100%|██████████| 7943/7943 [00:09<00:00, 853.82it/s] \n",
      "Processing hosp_id_day_keys: 100%|██████████| 7943/7943 [00:09<00:00, 831.29it/s] \n"
     ]
    }
   ],
   "source": [
    "df = merged_cohort[merged_cohort['on_vent_and_sedation']==1].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)  \n",
    "\n",
    "df['rank_sedation'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys'):\n",
    "    zero_mask = hosp_data['min_sedation_dose'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation'] = ranks.replace(0, np.nan)\n",
    "\n",
    "\n",
    "df['rank_sedation_non_ops'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys'):\n",
    "    zero_mask = hosp_data['min_sedation_dose_non_ops'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation_non_ops'] = ranks.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAT EHR all meds hard stop flaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hosp_id_day_keys for meds check: 100%|██████████| 7943/7943 [07:51<00:00, 16.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df['SAT_EHR_delivery'] = np.nan\n",
    "med_columns = ['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']\n",
    "\n",
    "# Use groupby and vectorized operations for meds check\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys for meds check'):\n",
    "    hosp_data_sorted = hosp_data.sort_values('event_time')\n",
    "    for index, row in hosp_data_sorted.iterrows():\n",
    "        if not np.isnan(row['rank_sedation']):\n",
    "            current_time = row['event_time']\n",
    "            thirty_min_forward = hosp_data_sorted[(hosp_data_sorted['event_time'] >= current_time) &\n",
    "                                                  (hosp_data_sorted['event_time'] <= current_time + pd.Timedelta(minutes=30))]\n",
    "            # Check if all med_columns are either NaN or 0 and device & location categories are \"imv\" and \"icu\" in this timeframe\n",
    "            if (\n",
    "                 (thirty_min_forward[med_columns].isna() | (thirty_min_forward[med_columns] == 0)).all(axis=None) and\n",
    "                 (thirty_min_forward['device_category_ffill'] == 'imv').all() and\n",
    "                 (thirty_min_forward['location_category_ffill'] == 'icu').all()\n",
    "            ):\n",
    "                df.at[index, 'SAT_EHR_delivery'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAT EHR all meds hard stop flaging (modified meds / non ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hosp_id_day_keys for meds check: 100%|██████████| 7943/7943 [10:08<00:00, 13.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df['SAT_modified_delivery'] = np.nan\n",
    "med_columns = ['propofol', 'lorazepam', 'midazolam']\n",
    "\n",
    "# Use groupby and vectorized operations for meds check\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Processing hosp_id_day_keys for meds check'):\n",
    "    hosp_data_sorted = hosp_data.sort_values('event_time')\n",
    "    for index, row in hosp_data_sorted.iterrows():\n",
    "        if not np.isnan(row['rank_sedation_non_ops']):\n",
    "            current_time = row['event_time']\n",
    "            thirty_min_forward = hosp_data_sorted[(hosp_data_sorted['event_time'] >= current_time) &\n",
    "                                                  (hosp_data_sorted['event_time'] <= current_time + pd.Timedelta(minutes=30))]\n",
    "\n",
    "            # Check if all med_columns are either NaN or 0 and device & location categories are \"imv\" and \"icu\" in this timeframe\n",
    "            if (\n",
    "                (thirty_min_forward[med_columns].isna() | (thirty_min_forward[med_columns] == 0)).all(axis=None) and\n",
    "                 (thirty_min_forward['device_category_ffill'] == 'imv').all() and\n",
    "                 (thirty_min_forward['location_category_ffill'] == 'icu').all()\n",
    "            ):\n",
    "                df.at[index, 'SAT_modified_delivery'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     22\u001b[39m times_df = times_df[\n\u001b[32m     23\u001b[39m     (times_df[\u001b[33m'\u001b[39m\u001b[33mdelta_to_ehr\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[32m0\u001b[39m) & (times_df[\u001b[33m'\u001b[39m\u001b[33mdelta_to_ehr\u001b[39m\u001b[33m'\u001b[39m] <= \u001b[32m1440\u001b[39m) &\n\u001b[32m     24\u001b[39m     (times_df[\u001b[33m'\u001b[39m\u001b[33mdelta_to_mod\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[32m0\u001b[39m) & (times_df[\u001b[33m'\u001b[39m\u001b[33mdelta_to_mod\u001b[39m\u001b[33m'\u001b[39m] <= \u001b[32m1440\u001b[39m)\n\u001b[32m     25\u001b[39m ]\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 5. Save CSVs\u001b[39;00m\n\u001b[32m     28\u001b[39m binned_df = pd.DataFrame({\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhour_bin\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mlabels\u001b[49m,\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcount_to_SAT_EHR_delivery\u001b[39m\u001b[33m'\u001b[39m: ehr_counts.values,\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcount_to_SAT_modified_delivery\u001b[39m\u001b[33m'\u001b[39m: mod_counts.values\n\u001b[32m     32\u001b[39m })\n\u001b[32m     33\u001b[39m binned_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/binned_delta_counts.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 6. Bin into hourly intervals\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Identify initial sat failure events and delivery events\n",
    "mask_initial = (df['sat_delivery_pass_fail'] == 1) | (df['sat_screen_pass_fail'] == 1)\n",
    "mask_ehr = df['SAT_EHR_delivery'] == 1\n",
    "mask_mod = df['SAT_modified_delivery'] == 1\n",
    "\n",
    "initial_times = df[mask_initial].groupby('hosp_id_day_key')['event_time'].min().rename('initial_time')\n",
    "ehr_times = df[mask_ehr].groupby('hosp_id_day_key')['event_time'].min().rename('ehr_time')\n",
    "mod_times = df[mask_mod].groupby('hosp_id_day_key')['event_time'].min().rename('mod_time')\n",
    "\n",
    "# 2. Merge into a single DataFrame and drop incomplete cases\n",
    "times_df = pd.concat([initial_times, ehr_times, mod_times], axis=1).dropna()\n",
    "\n",
    "# 3. Convert event_time columns to datetime\n",
    "for col in ['initial_time', 'ehr_time', 'mod_time']:\n",
    "    times_df[col] = pd.to_datetime(times_df[col])\n",
    "\n",
    "# 4. Compute deltas in minutes\n",
    "times_df['delta_to_ehr'] = (times_df['ehr_time'] - times_df['initial_time']).dt.total_seconds() / 60\n",
    "times_df['delta_to_mod'] = (times_df['mod_time'] - times_df['initial_time']).dt.total_seconds() / 60\n",
    "\n",
    "# 5. Filter deltas to positive values within 24 hours (0–1440 minutes)\n",
    "times_df = times_df[\n",
    "    (times_df['delta_to_ehr'] >= 0) & (times_df['delta_to_ehr'] <= 1440) &\n",
    "    (times_df['delta_to_mod'] >= 0) & (times_df['delta_to_mod'] <= 1440)\n",
    "]\n",
    "\n",
    "# 5. Save CSVs\n",
    "binned_df = pd.DataFrame({\n",
    "    'hour_bin': labels,\n",
    "    'count_to_SAT_EHR_delivery': ehr_counts.values,\n",
    "    'count_to_SAT_modified_delivery': mod_counts.values\n",
    "})\n",
    "binned_df.to_csv(f'{directory_path}/binned_delta_counts.csv', index=False)\n",
    "\n",
    "# 6. Bin into hourly intervals\n",
    "bins = list(range(0, 24*60 + 1, 60))  # [0, 60, 120, ..., 1440]\n",
    "labels = [f'{i}-{i+1}hr' for i in range(24)]\n",
    "\n",
    "ehr_binned = pd.cut(times_df['delta_to_ehr'], bins=bins, labels=labels, right=False)\n",
    "mod_binned = pd.cut(times_df['delta_to_mod'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "ehr_counts = ehr_binned.value_counts().sort_index()\n",
    "mod_counts = mod_binned.value_counts().sort_index()\n",
    "\n",
    "# 7. Plot both lines hour-wise\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(labels, ehr_counts.values, marker='o', label='To SAT_EHR_delivery')\n",
    "plt.plot(labels, mod_counts.values, marker='s', label='To SAT_modified_delivery')\n",
    "plt.xlabel('Hours since initial failure event')\n",
    "plt.ylabel('Count of Hospital-Day Keys')\n",
    "plt.title('Hourly Distribution of Time to EHR and Modified Deliveries')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Icu los calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_los = cohort[['hospitalization_id', 'event_time', 'location_category_ffill']]\n",
    "icu_los = icu_los.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "icu_los['segment'] = (icu_los['location_category_ffill'] != icu_los['location_category_ffill'].shift()).cumsum()\n",
    "\n",
    "icu_segments = icu_los[icu_los['location_category_ffill'].str.lower() == 'icu'].groupby(\n",
    "    ['hospitalization_id', 'segment']\n",
    ").agg(\n",
    "    location_start=('event_time', 'first'),\n",
    "    location_end=('event_time', 'last')\n",
    ").reset_index()\n",
    "\n",
    "icu_segments['los_days'] = (icu_segments['location_end'] - icu_segments['location_start']).dt.total_seconds() / (24 * 3600)\n",
    "icu_los_per_encounter = icu_segments[['hospitalization_id', 'los_days']]\n",
    "\n",
    "total_icu_los_per_hosp = icu_los_per_encounter.groupby('hospitalization_id', as_index=False).agg(\n",
    "    ICU_LOS=('los_days', 'sum')\n",
    ")\n",
    "total_icu_los_per_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last dishcharge hosptial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hosp = cohort[['hospitalization_id', 'event_time', 'hospital_id']]\n",
    "\n",
    "last_hosp = last_hosp.sort_values(by=['hospitalization_id','event_time'], ascending=False).groupby(\n",
    "    ['hospitalization_id'], as_index=False\n",
    ").agg(({'hospital_id': 'first'})).reset_index(drop=True)\n",
    "last_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table one df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7943, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = df[['patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "       'age_at_admission', 'discharge_category', 'sex_category',\n",
    "       'race_category', 'ethnicity_category','hosp_id_day_key']].drop_duplicates()\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7943, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = pd.merge(main, total_icu_los_per_hosp, on='hospitalization_id', how='left')\n",
    "main = pd.merge(main, last_hosp, on='hospitalization_id', how='left')\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7943, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to group by\n",
    "group_cols = [\n",
    " 'hosp_id_day_key'\n",
    "]\n",
    "\n",
    "max_cols = ['sat_screen_pass_fail','sat_delivery_pass_fail','SAT_EHR_delivery', 'SAT_modified_delivery', 'eligible_event']\n",
    "agg_dict = {col: 'max' for col in max_cols}\n",
    "\n",
    "df_grouped = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "df_grouped = df_grouped.sort_values('hosp_id_day_key').reset_index(drop=True)\n",
    "\n",
    "df_grouped['sat_flowsheet_delivery_flag'] = np.where(\n",
    "    (\n",
    "        (df_grouped['sat_screen_pass_fail'] == 1) |\n",
    "        (df_grouped['sat_delivery_pass_fail'] == 1)\n",
    "    ) &\n",
    "    (df_grouped['eligible_event'] == 1),\n",
    "    1,  # Flag is set to 1 (True) if conditions are met\n",
    "    np.nan   # Flag nan\n",
    ")\n",
    "\n",
    "final_df = main.merge(df_grouped, on='hosp_id_day_key', how='inner')\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sat_delivery_pass_fail\n",
      "1.0    2386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "sat_screen_pass_fail\n",
      "1.0    4234\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SAT_EHR_delivery\n",
      "1.0    2907\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SAT_modified_delivery\n",
      "1.0    5535\n",
      "Name: count, dtype: int64\n",
      "\n",
      "eligible_event\n",
      "1.0    7943\n",
      "Name: count, dtype: int64\n",
      "\n",
      "sat_flowsheet_delivery_flag\n",
      "1.0    4248\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in ['sat_delivery_pass_fail', 'sat_screen_pass_fail', 'SAT_EHR_delivery',\n",
    "       'SAT_modified_delivery', 'eligible_event',\n",
    "       'sat_flowsheet_delivery_flag']:\n",
    "    print(final_df[x].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m metrics_list = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mSAT_EHR_delivery\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSAT_modified_delivery\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     x = \u001b[43mfinal_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msat_flowsheet_delivery_flag\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m     y = final_df[z]\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# compute confusion matrix\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# DataFrame to collect all metrics\n",
    "metrics_list = []\n",
    "\n",
    "for z in ['SAT_EHR_delivery', 'SAT_modified_delivery']:\n",
    "    x = final_df['sat_flowsheet_delivery_flag']\n",
    "    y = final_df[z]\n",
    "\n",
    "    # compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(x, y).ravel()\n",
    "\n",
    "    # compute derived metrics\n",
    "    accuracy    = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision   = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall      = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1          = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) else 0\n",
    "\n",
    "    # plot and save confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        x, y,\n",
    "        display_labels=[\"No Delivery\", \"Delivery\"],\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=None\n",
    "    )\n",
    "    disp.ax_.set_title(f\"Confusion Matrix for {z}\")\n",
    "    fig = disp.figure_\n",
    "    plot_path = os.path.join(directory_path, f\"confusion_matrix_{z}.png\")\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # print metrics\n",
    "    print(f\"Column      : {z}\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "\n",
    "    # collect metrics in a dict\n",
    "    metrics_dict = {\n",
    "        \"Column\": z,\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Plot Path\": plot_path\n",
    "    }\n",
    "    metrics_list.append(metrics_dict)\n",
    "\n",
    "# convert to DataFrame and save\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(os.path.join(directory_path, \"delivery_metrics_summary.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../output/intermediate/final_df_SAT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table one print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex_category', 'race_category', 'ethnicity_category','discharge_category']\n",
    "non_categorical_columns = ['age_at_admission',  'ICU_LOS', 'Inpatient_LOS']\n",
    "\n",
    "final_df['admission_dttm'] = pd.to_datetime(final_df['admission_dttm'],utc=True)\n",
    "final_df['discharge_dttm'] = pd.to_datetime(final_df['discharge_dttm'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Missing           Overall\n",
      "n                                                                                               1922\n",
      "sex_category, n (%)              Female                                                   844 (43.9)\n",
      "                                 Male                                                    1078 (56.1)\n",
      "race_category, n (%)             American Indian or Alaska Native                            3 (0.2)\n",
      "                                 Asian                                                      82 (4.3)\n",
      "                                 Black or African American                                673 (35.0)\n",
      "                                 Native Hawaiian or Other Pacific Islander                   3 (0.2)\n",
      "                                 Other                                                    359 (18.7)\n",
      "                                 Unknown                                                    43 (2.2)\n",
      "                                 White                                                    759 (39.5)\n",
      "ethnicity_category, n (%)        Hispanic                                                 387 (20.1)\n",
      "                                 Non-Hispanic                                            1501 (78.1)\n",
      "                                 Unknown                                                    34 (1.8)\n",
      "discharge_category, n (%)        Acute Care Hospital                                        17 (0.9)\n",
      "                                 Acute Inpatient Rehab Facility                           289 (15.0)\n",
      "                                 Against Medical Advice (AMA)                               16 (0.8)\n",
      "                                 Assisted Living                                             1 (0.1)\n",
      "                                 Chemical Dependency                                         1 (0.1)\n",
      "                                 Expired                                                  515 (26.8)\n",
      "                                 Home                                                     737 (38.3)\n",
      "                                 Hospice                                                    45 (2.3)\n",
      "                                 Long Term Care Hospital (LTACH)                            52 (2.7)\n",
      "                                 Missing                                                   102 (5.3)\n",
      "                                 Other                                                       1 (0.1)\n",
      "                                 Psychiatric Hospital                                        7 (0.4)\n",
      "                                 Skilled Nursing Facility (SNF)                            139 (7.2)\n",
      "age_at_admission, median [Q1,Q3]                                                 0  62.0 [50.0,71.0]\n",
      "ICU_LOS, median [Q1,Q3]                                                          0    7.0 [3.8,13.2]\n",
      "Inpatient_LOS, median [Q1,Q3]                                                    0   12.8 [7.4,21.9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SAT FLAG Table 1\n",
    "\n",
    "\n",
    "sat_flow_t1 = final_df[final_df['sat_flowsheet_delivery_flag'] == 1][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_flow_t1['Inpatient_LOS'] = (sat_flow_t1['discharge_dttm'] - sat_flow_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_flow_t1)>1:\n",
    "    table1 = TableOne(sat_flow_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table1.to_csv(f'{directory_path}/table1_sat_flowhseet_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Missing           Overall\n",
      "n                                                                                               2286\n",
      "sex_category, n (%)              Female                                                  1027 (44.9)\n",
      "                                 Male                                                    1259 (55.1)\n",
      "race_category, n (%)             American Indian or Alaska Native                            4 (0.2)\n",
      "                                 Asian                                                      90 (3.9)\n",
      "                                 Black or African American                                808 (35.3)\n",
      "                                 Native Hawaiian or Other Pacific Islander                   3 (0.1)\n",
      "                                 Other                                                    435 (19.0)\n",
      "                                 Unknown                                                    57 (2.5)\n",
      "                                 White                                                    889 (38.9)\n",
      "ethnicity_category, n (%)        Hispanic                                                 452 (19.8)\n",
      "                                 Non-Hispanic                                            1792 (78.4)\n",
      "                                 Unknown                                                    42 (1.8)\n",
      "discharge_category, n (%)        Acute Care Hospital                                        17 (0.7)\n",
      "                                 Acute Inpatient Rehab Facility                           342 (15.0)\n",
      "                                 Against Medical Advice (AMA)                               19 (0.8)\n",
      "                                 Assisted Living                                             1 (0.0)\n",
      "                                 Chemical Dependency                                         1 (0.0)\n",
      "                                 Expired                                                  700 (30.6)\n",
      "                                 Home                                                     813 (35.6)\n",
      "                                 Hospice                                                    52 (2.3)\n",
      "                                 Long Term Care Hospital (LTACH)                            54 (2.4)\n",
      "                                 Missing                                                   117 (5.1)\n",
      "                                 Other                                                       1 (0.0)\n",
      "                                 Psychiatric Hospital                                        8 (0.3)\n",
      "                                 Skilled Nursing Facility (SNF)                            161 (7.0)\n",
      "age_at_admission, median [Q1,Q3]                                                 0  62.0 [50.0,71.0]\n",
      "ICU_LOS, median [Q1,Q3]                                                          0    6.7 [3.6,12.7]\n",
      "Inpatient_LOS, median [Q1,Q3]                                                    0   12.1 [7.0,21.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SAT EHR FLAG Table 1\n",
    "\n",
    "sat_ehr_t1 = final_df[(final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_ehr_t1['Inpatient_LOS'] = (sat_ehr_t1['discharge_dttm'] - sat_ehr_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_ehr_t1)>1:\n",
    "    table2 = TableOne(sat_ehr_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table2.to_csv(f'{directory_path}/table1_sat_ehr_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Missing           Overall\n",
      "n                                                                                               2525\n",
      "sex_category, n (%)              Female                                                  1129 (44.7)\n",
      "                                 Male                                                    1396 (55.3)\n",
      "race_category, n (%)             American Indian or Alaska Native                            4 (0.2)\n",
      "                                 Asian                                                      98 (3.9)\n",
      "                                 Black or African American                                896 (35.5)\n",
      "                                 Native Hawaiian or Other Pacific Islander                   3 (0.1)\n",
      "                                 Other                                                    465 (18.4)\n",
      "                                 Unknown                                                    68 (2.7)\n",
      "                                 White                                                    991 (39.2)\n",
      "ethnicity_category, n (%)        Hispanic                                                 493 (19.5)\n",
      "                                 Non-Hispanic                                            1983 (78.5)\n",
      "                                 Unknown                                                    49 (1.9)\n",
      "discharge_category, n (%)        Acute Care Hospital                                        18 (0.7)\n",
      "                                 Acute Inpatient Rehab Facility                           380 (15.0)\n",
      "                                 Against Medical Advice (AMA)                               23 (0.9)\n",
      "                                 Assisted Living                                             1 (0.0)\n",
      "                                 Chemical Dependency                                         1 (0.0)\n",
      "                                 Expired                                                  757 (30.0)\n",
      "                                 Home                                                     911 (36.1)\n",
      "                                 Hospice                                                    58 (2.3)\n",
      "                                 Long Term Care Hospital (LTACH)                            64 (2.5)\n",
      "                                 Missing                                                   127 (5.0)\n",
      "                                 Other                                                       1 (0.0)\n",
      "                                 Psychiatric Hospital                                        9 (0.4)\n",
      "                                 Skilled Nursing Facility (SNF)                            175 (6.9)\n",
      "age_at_admission, median [Q1,Q3]                                                 0  62.0 [50.0,71.0]\n",
      "ICU_LOS, median [Q1,Q3]                                                          0    6.6 [3.5,12.4]\n",
      "Inpatient_LOS, median [Q1,Q3]                                                    0   11.9 [6.8,20.6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### all Table 1\n",
    "\n",
    "all_t1 = final_df[[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "all_t1['Inpatient_LOS'] = (all_t1['discharge_dttm'] - all_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(all_t1)>1:\n",
    "    table3 = TableOne(all_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table3.to_csv(f'{directory_path}/table1_all_t1_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per hospital stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "73e5db83-19cb-4ac0-867f-e9a0a4f7f56b",
       "rows": [
        [
         "Site_Name_Hosp",
         "RUSH_RUMC"
        ],
        [
         "%_of_SAT_flowsheet_delivery_flag",
         "53.48105249905577"
        ],
        [
         "%_of_SAT_modified_delivery",
         "69.6839984892358"
        ],
        [
         "%_of_SAT_EHR_delivery",
         "36.59826262117588"
        ],
        [
         "eligible_event_count",
         "7943"
        ],
        [
         "sat_flowsheet_delivery_flag_count",
         "4248"
        ],
        [
         "SAT_modified_delivery_count",
         "5535"
        ],
        [
         "SAT_EHR_delivery_count",
         "2907"
        ],
        [
         "SAT_EHR_unique_patients",
         "1851"
        ],
        [
         "SAT_EHR_unique_hospitalizations",
         "1908"
        ],
        [
         "SAT_modified_unique_patients",
         "2202"
        ],
        [
         "SAT_modified_unique_hospitalizations",
         "2286"
        ],
        [
         "SAT_EHR_modified_unique_patients",
         "2202"
        ],
        [
         "SAT_EHR_modified_unique_hospitalizations",
         "2286"
        ],
        [
         "SAT_flowsheet_unique_patients",
         "1852"
        ],
        [
         "SAT_flowsheet_unique_hospitalizations",
         "1922"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Site_Name_Hosp</th>\n",
       "      <td>RUSH_RUMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%_of_SAT_flowsheet_delivery_flag</th>\n",
       "      <td>53.481052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%_of_SAT_modified_delivery</th>\n",
       "      <td>69.683998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%_of_SAT_EHR_delivery</th>\n",
       "      <td>36.598263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eligible_event_count</th>\n",
       "      <td>7943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sat_flowsheet_delivery_flag_count</th>\n",
       "      <td>4248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_modified_delivery_count</th>\n",
       "      <td>5535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_EHR_delivery_count</th>\n",
       "      <td>2907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_EHR_unique_patients</th>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_EHR_unique_hospitalizations</th>\n",
       "      <td>1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_modified_unique_patients</th>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_modified_unique_hospitalizations</th>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_EHR_modified_unique_patients</th>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_EHR_modified_unique_hospitalizations</th>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_flowsheet_unique_patients</th>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAT_flowsheet_unique_hospitalizations</th>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0\n",
       "Site_Name_Hosp                            RUSH_RUMC\n",
       "%_of_SAT_flowsheet_delivery_flag          53.481052\n",
       "%_of_SAT_modified_delivery                69.683998\n",
       "%_of_SAT_EHR_delivery                     36.598263\n",
       "eligible_event_count                           7943\n",
       "sat_flowsheet_delivery_flag_count              4248\n",
       "SAT_modified_delivery_count                    5535\n",
       "SAT_EHR_delivery_count                         2907\n",
       "SAT_EHR_unique_patients                        1851\n",
       "SAT_EHR_unique_hospitalizations                1908\n",
       "SAT_modified_unique_patients                   2202\n",
       "SAT_modified_unique_hospitalizations           2286\n",
       "SAT_EHR_modified_unique_patients               2202\n",
       "SAT_EHR_modified_unique_hospitalizations       2286\n",
       "SAT_flowsheet_unique_patients                  1852\n",
       "SAT_flowsheet_unique_hospitalizations          1922"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store each hospital's data\n",
    "data_list = []\n",
    "\n",
    "# Iterate over unique hospital IDs as strings\n",
    "for x in final_df['hospital_id'].astype(str).unique():\n",
    "    # Calculate counts based on specific conditions\n",
    "    eligible_event_count = final_df[(final_df['eligible_event'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    sat_flowsheet_delivery_flag_count = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    SAT_modified_delivery_count = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "    SAT_EHR_delivery_count = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)].shape[0]\n",
    "\n",
    "    SAT_EHR_uni_pats = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_EHR_hosp = final_df[(final_df['SAT_EHR_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_modified_uni_pats = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_modified_hosp = final_df[(final_df['SAT_modified_delivery'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_EHR_modified_uni_pats = final_df[((final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_EHR_modified_hosp = final_df[((final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_flowsheet_uni_pats = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)]['patient_id'].nunique()\n",
    "    SAT_flowsheet_hosp = final_df[(final_df['sat_flowsheet_delivery_flag'] == 1) & (final_df['hospital_id'].astype(str) == x)]['hospitalization_id'].nunique()\n",
    "\n",
    "    # Safeguard against division by zero\n",
    "    if eligible_event_count > 0:\n",
    "        percent_sat_flowsheet_delivery_flag = (sat_flowsheet_delivery_flag_count / eligible_event_count) * 100\n",
    "        percent_SAT_modified_delivery = (SAT_modified_delivery_count / eligible_event_count) * 100\n",
    "        percent_SAT_EHR_delivery = (SAT_EHR_delivery_count / eligible_event_count) * 100\n",
    "    else:\n",
    "        percent_sat_flowsheet_delivery_flag = 0\n",
    "        percent_SAT_modified_delivery = 0\n",
    "        percent_SAT_EHR_delivery = 0\n",
    "\n",
    "    # Append the data for this hospital to the list\n",
    "    data_list.append({\n",
    "        'Site_Name_Hosp': pc.helper[\"site_name\"] + '_' + x,  \n",
    "        '%_of_SAT_flowsheet_delivery_flag': percent_sat_flowsheet_delivery_flag,\n",
    "        '%_of_SAT_modified_delivery': percent_SAT_modified_delivery,\n",
    "        '%_of_SAT_EHR_delivery': percent_SAT_EHR_delivery,\n",
    "        'eligible_event_count': eligible_event_count,\n",
    "        'sat_flowsheet_delivery_flag_count': sat_flowsheet_delivery_flag_count,\n",
    "        'SAT_modified_delivery_count': SAT_modified_delivery_count,\n",
    "        'SAT_EHR_delivery_count': SAT_EHR_delivery_count,\n",
    "\n",
    "        'SAT_EHR_unique_patients': SAT_EHR_uni_pats,\n",
    "        'SAT_EHR_unique_hospitalizations': SAT_EHR_hosp,\n",
    "        'SAT_modified_unique_patients': SAT_modified_uni_pats,\n",
    "        'SAT_modified_unique_hospitalizations': SAT_modified_hosp,\n",
    "        'SAT_EHR_modified_unique_patients': SAT_EHR_modified_uni_pats,\n",
    "        'SAT_EHR_modified_unique_hospitalizations': SAT_EHR_modified_hosp,\n",
    "        'SAT_flowsheet_unique_patients': SAT_flowsheet_uni_pats, \n",
    "        'SAT_flowsheet_unique_hospitalizations': SAT_flowsheet_hosp   \n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "final_data_df = pd.DataFrame(data_list)\n",
    "final_data_df.to_csv(f'{directory_path}/sat_stats_{pc.helper[\"site_name\"]}.csv',index=False)\n",
    "# Display the final DataFrame\n",
    "final_data_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Table 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate functions\n",
    "def documented(series):\n",
    "    return \"Documented\" if series.notna().any() else \"Not Documented\"\n",
    "\n",
    "def age_bucket(mean_age):\n",
    "    if pd.isna(mean_age):\n",
    "        return None\n",
    "    elif mean_age < 40:\n",
    "        return \"18-39\"\n",
    "    elif mean_age < 60:\n",
    "        return \"40-59\"\n",
    "    elif mean_age < 80:\n",
    "        return \"60-79\"\n",
    "    else:\n",
    "        return \"80+\"\n",
    "\n",
    "# Clean 'language_name' to only \"English\", \"Spanish\", or \"Other\"\n",
    "def categorize_language(lang):\n",
    "    if re.search(r'english', str(lang), re.IGNORECASE):\n",
    "        return 'English'\n",
    "    elif re.search(r'spanish', str(lang), re.IGNORECASE):\n",
    "        return 'Spanish'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "t1_col = [\n",
    "    \"patient_id\",\n",
    "    \"hospitalization_id\",\n",
    "    \"hosp_id_day_key\",\n",
    "    \"age_at_admission\",    \"sex_category\",    \"race_category\",    \"ethnicity_category\",    \"language_name\",    \"weight_kg\",\n",
    "    \"height_cm\", \"cisatracurium\",    \"vecuronium\",    \"rocuronium\",    \"dobutamine\",    \"dopamine\",    \"epinephrine\",\n",
    "    \"fentanyl\",    \"hydromorphone\",    \"isoproterenol\",    \"lorazepam\",    \"midazolam\",    \"milrinone\",    \"morphine\",\n",
    "    \"norepinephrine\",    \"phenylephrine\",    \"propofol\",    \"vasopressin\",    \"angiotensin\",     \"rass\", \"gcs_total\"\n",
    "]\n",
    "\n",
    "medication_columns = [\n",
    "    \"rass\", \"gcs_total\", \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "demographic_columns = [\"sex_category\", \"race_category\", \"ethnicity_category\", \"language_name\"]\n",
    "\n",
    "continuous_cols = [\n",
    "    \"rass\", \"gcs_total\", \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\",\n",
    "    \"angiotensin\", \"bmi\"\n",
    "]\n",
    "\n",
    "drugs = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Apply the transformation\n",
    "t1_cohort[drugs] = t1_cohort[drugs].applymap(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "t1_cohort['bmi'] = t1_cohort['weight_kg'] / ((t1_cohort['height_cm'] / 100) ** 2)\n",
    "\n",
    "# Apply the function to 'language_name'\n",
    "t1_cohort['language_name'] = t1_cohort['language_name'].apply(categorize_language)\n",
    "t1_cohort['rass'] = t1_cohort['rass'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By ID for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['hospitalization_id', 'patient_id']:\n",
    "    t1_summary = t1_cohort.groupby(x).agg(\n",
    "        {\n",
    "            \"age_at_admission\": \"mean\",\n",
    "            **{col: documented for col in medication_columns},\n",
    "            **{col: \"first\" for col in demographic_columns}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    t1_summary[\"age_bucket\"] = t1_summary[\"age_at_admission\"].apply(age_bucket)\n",
    "    t1_summary = t1_summary.drop(columns=[\"age_at_admission\"])\n",
    "    t1_summary = t1_summary.reset_index()\n",
    "\n",
    "    summary_df = t1code.manual_categorical_tableone(\n",
    "        t1_summary, \n",
    "        medication_columns + demographic_columns + [\"age_bucket\"]\n",
    "    )\n",
    "\n",
    "    if x == 'hospitalization_id':\n",
    "        summary_df.to_csv(f\"{directory_path}/table1_hospitalization_id_categorical.csv\", index=False)\n",
    "    else:\n",
    "        summary_df.to_csv(f\"{directory_path}/table1_patient_id_categorical.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By ID for Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalization_summary = None\n",
    "patient_summary = None\n",
    "\n",
    "hosp = (\n",
    "    t1_cohort\n",
    "      .groupby(\"hospitalization_id\")\n",
    "      .agg(\n",
    "        {\n",
    "          **{c: \"median\" for c in continuous_cols}\n",
    "        }\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "patient = (\n",
    "    t1_cohort\n",
    "      .groupby(\"patient_id\")\n",
    "      .agg(\n",
    "        {\n",
    "          **{c: \"median\" for c in continuous_cols}\n",
    "        }\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Build for hospitalization level and patient level\n",
    "hospitalization_summary = t1code.manual_tableone(hosp, continuous_cols)\n",
    "patient_summary = t1code.manual_tableone(patient, continuous_cols)\n",
    "\n",
    "hospitalization_summary.to_csv(f\"{directory_path}/table1_hospitalization_id_continuous.csv\", index=False)\n",
    "patient_summary.to_csv(f\"{directory_path}/table1_patient_id_continuous.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By Days for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:29<00:00,  9.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(\n",
    "    [\n",
    "        \"eligible_event\",\n",
    "        \"SAT_EHR_delivery\",\n",
    "        \"SAT_modified_delivery\"\n",
    "    ]\n",
    "):\n",
    "    ids_to_use = final_df[final_df[x]==1].hosp_id_day_key.unique()\n",
    "    # Groupby aggregation by hospitalization_id\n",
    "    t1_summary = t1_cohort[t1_cohort['hosp_id_day_key'].isin(ids_to_use)].groupby(\"hosp_id_day_key\").agg(\n",
    "        {\n",
    "            \"age_at_admission\": \"mean\",\n",
    "            **{col: documented for col in medication_columns},\n",
    "            **{col: \"first\" for col in demographic_columns},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Apply age bucketing\n",
    "    t1_summary[\"age_bucket\"] = t1_summary[\"age_at_admission\"].apply(age_bucket)\n",
    "\n",
    "    # Drop raw age if you don't need it\n",
    "    t1_summary = t1_summary.drop(columns=[\"age_at_admission\"])\n",
    "\n",
    "    # Reset index if needed\n",
    "    t1_summary = t1_summary.reset_index()\n",
    "\n",
    "    summary_df = t1code.manual_categorical_tableone(\n",
    "        t1_summary, \n",
    "        medication_columns + demographic_columns + [\"age_bucket\"]\n",
    "    )\n",
    "    summary_df.to_csv(f\"{directory_path}/table1_{x}_categorical.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(\n",
    "    [\n",
    "        \"eligible_event\",\n",
    "        \"SAT_EHR_delivery\",\n",
    "        \"SAT_modified_delivery\",\n",
    "    ]\n",
    "):\n",
    "    # --- filter to only the days in this subcohort\n",
    "    ids = final_df.loc[final_df[x] == 1, \"hosp_id_day_key\"].unique()\n",
    "    sub = t1_cohort[t1_cohort[\"hosp_id_day_key\"].isin(ids)]\n",
    "\n",
    "    # --- 1) Day-level medians + flags + demographics\n",
    "    day_summary = (\n",
    "        sub.groupby(\"hosp_id_day_key\")\n",
    "        .agg({**{c: \"median\" for c in continuous_cols}})\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary_df = t1code.manual_tableone(day_summary, continuous_cols)\n",
    "    summary_df.to_csv(f\"{directory_path}/table1_{x}_continuous.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank You!!! keep latest timestamp files and upload to box :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
