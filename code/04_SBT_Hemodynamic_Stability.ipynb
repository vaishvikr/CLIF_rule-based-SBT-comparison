{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyCLIF as pc\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "cohort = pd.read_csv('../output/intermediate/study_cohort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full directory path\n",
    "directory_path = os.path.join('../output/final/', pc.helper['site_name'], 'Hemodynamic_Stability')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc.helper['site_name']=='RUSH':\n",
    "    cohort.loc[cohort['sbt_timepoint'] == '3-5 minute evaluation', 'pressure_support_set'] = 6.1\n",
    "    cohort.loc[cohort['sbt_timepoint'] == '3-5 minute evaluation', 'mode_category'] = 'Pressure Support/CPAP'\n",
    "    print('its a rush thing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility Flag making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device Fillforward After Waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vasoactive-> if the meds are missing from site then fill NaN\n",
    "active_vasoactive_n_col = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\",\"vasopressin\", \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\"\n",
    "]\n",
    "for col in active_vasoactive_n_col:\n",
    "    if col not in cohort.columns:\n",
    "        cohort[col] = np.nan\n",
    "\n",
    "# Ensure all time columns are in datetime format\n",
    "cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "cohort['admission_dttm'] = pc.getdttm(cohort['admission_dttm'])\n",
    "cohort['discharge_dttm'] = pc.getdttm(cohort['discharge_dttm'])\n",
    "\n",
    "# Ensure the data is sorted by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "cohort['device_category'] = cohort['device_category'].str.lower()\n",
    "cohort['mode_category'] = cohort['mode_category'].str.lower()\n",
    "\n",
    "# Fill forward the meds by hospitalization columns by 'hosp_id'\n",
    "cohort[['device_category', 'mode_category', 'mode_name',\n",
    "        'location_category','hospital_id']] = cohort.groupby('hospitalization_id')[\n",
    "    ['device_category', 'mode_category','mode_name',\n",
    "     'location_category','hospital_id']\n",
    "].ffill()\n",
    "\n",
    "cohort[[\"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\",\n",
    "    \"vasopressin\", \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\"]] = cohort.groupby('hospitalization_id')[\n",
    "    [\"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\",\n",
    "    \"vasopressin\", \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\"]\n",
    "].ffill()\n",
    "\n",
    "cohort[[\"fio2_set\",\"peep_set\",\"spo2\",'pressure_support_set']] = cohort.groupby('hospitalization_id')[\n",
    "    [\"fio2_set\",\"peep_set\",\"spo2\",'pressure_support_set']\n",
    "].ffill()\n",
    "\n",
    "cohort[['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'angiotensin', 'vasopressin']] = \\\n",
    "    cohort[['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'angiotensin', 'vasopressin']].fillna(0)\n",
    "\n",
    "cohort['NEE'] = cohort['norepinephrine'] + cohort['epinephrine'] + (cohort['phenylephrine']/10) + (cohort['vasopressin']*2.5) + (cohort['dopamine']/100) + (cohort['angiotensin']*10)\n",
    "\n",
    "cohort[\"Hemodynamic_Stability_by_NEE\"] = (\n",
    "    ((cohort[\"NEE\"] <= 0.2))\n",
    ").astype(int)\n",
    "\n",
    "# Define Respiratory Stability Flag\n",
    "cohort[\"Respiratory_Stability\"] = (\n",
    "    (cohort[\"fio2_set\"] <= 0.5) &\n",
    "    (cohort[\"peep_set\"] <= 8) &\n",
    "    (cohort[\"spo2\"] >= 88)\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBT Eligibility Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cohort_conditions(cohort):\n",
    "    # --- Preliminary processing ---\n",
    "    # Ensure event_time is datetime and sort the dataframe\n",
    "    cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "    cohort = cohort.sort_values(['hospitalization_id', 'event_time']).reset_index(drop=False)\n",
    "    \n",
    "    # IMV flag\n",
    "    cohort['IMV_flag'] = (\n",
    "        (cohort['device_category'] == 'imv') &\n",
    "        (cohort['location_category'] == 'icu') &\n",
    "        (cohort['Hemodynamic_Stability_by_NEE'] == 1)\n",
    "    )\n",
    "    \n",
    "    # --- Prepare new flag columns ---\n",
    "    # For Condition 1, record the event_time when the threshold is reached.\n",
    "    cohort['IMV_Controlled_met_time'] = pd.NaT\n",
    "    # New flag for eligible day (1 if condition 1 is met that day, else 0)\n",
    "    cohort['eligible_day'] = 0\n",
    "    \n",
    "    # For grouping by day, use the normalized event_time (midnight)\n",
    "    cohort['current_day'] = cohort['event_time'].dt.normalize()\n",
    "    \n",
    "    # Build a dictionary of full hospitalization data to avoid repeated filtering.\n",
    "    hosp_groups = {\n",
    "        hosp_id: df.copy().sort_values('event_time')\n",
    "        for hosp_id, df in cohort.groupby('hospitalization_id')\n",
    "    }\n",
    "    \n",
    "    # --- Define thresholds and time windows ---\n",
    "    cond1_threshold = pd.Timedelta(hours=6)  # Condition 1: 6 cumulative hours\n",
    "   \n",
    "    # For Condition 1: window is 10 PM (previous day) to 6 AM (current day)\n",
    "    cond1_window_start_offset = pd.Timedelta(hours=22) - pd.Timedelta(days=1)  # previous day 10 PM\n",
    "    cond1_window_end_offset = pd.Timedelta(hours=6)  # current day 6 AM\n",
    "    \n",
    "    # --- Process each hospitalization and day ---\n",
    "    # Group by hospitalization and current day\n",
    "    groups = cohort.groupby(['hospitalization_id', 'current_day'])\n",
    "    \n",
    "    for (hosp_id, curr_day), day_group in tqdm(groups, desc=\"Processing each Hosp & Day\"):\n",
    "        # --- Condition 1: IMV in controlled mode ---\n",
    "        # Define window for condition 1 based on the current day\n",
    "        cond1_start = curr_day + cond1_window_start_offset\n",
    "        cond1_end = curr_day + cond1_window_end_offset\n",
    "        \n",
    "        # Use full hospitalization data so events before midnight can contribute.\n",
    "        hosp_df = hosp_groups[hosp_id]\n",
    "        cond1_df = hosp_df[(hosp_df['event_time'] >= cond1_start) & (hosp_df['event_time'] <= cond1_end)].copy()\n",
    "        if cond1_df.empty:\n",
    "            continue  # no events in this window\n",
    "        \n",
    "        if not cond1_df['IMV_flag'].any():\n",
    "            continue\n",
    "        \n",
    "        # Identify contiguous segments where IMV_flag is True.\n",
    "        cond1_df['seg'] = (cond1_df['IMV_flag'] != cond1_df['IMV_flag'].shift()).cumsum()\n",
    "        valid_segs = cond1_df[cond1_df['IMV_flag']].groupby('seg')\n",
    "        \n",
    "        cond1_met = False  # flag indicating if condition 1 was met\n",
    "        for seg_id, seg_df in valid_segs:\n",
    "            seg_df = seg_df.sort_values('event_time')\n",
    "            seg_df['duration'] = seg_df['event_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "            seg_df['cum_duration'] = seg_df['duration'].cumsum()\n",
    "            if seg_df['cum_duration'].iloc[-1] >= cond1_threshold:\n",
    "                # Find the first row where the cumulative duration reaches the threshold.\n",
    "                flag_row = seg_df[seg_df['cum_duration'] >= cond1_threshold].iloc[0]\n",
    "                flag_idx = flag_row.name  # this is the original index in hosp_df (and cohort)\n",
    "                flag_time = flag_row['event_time']\n",
    "                cohort.loc[flag_idx, 'IMV_Controlled_met_time'] = flag_time\n",
    "                cond1_met = True\n",
    "                break  # Only the first qualifying segment for this day is flagged.\n",
    "        \n",
    "        # --- Eligible Day Flag ---\n",
    "        # If condition 1 is met for the day, mark all rows of this day as eligible_day = 1.\n",
    "        if cond1_met:\n",
    "            cohort.loc[day_group.index, 'eligible_day'] = 1\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# Example usage:\n",
    "final_df = process_cohort_conditions(cohort)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print('By n = Days')\n",
    "total_days = final_df['hosp_id_day_key'].nunique()\n",
    "print('Total number of days for eval in cohort:', total_days)\n",
    "total_vent_days = final_df[final_df['device_category'] == 'imv' ]['hosp_id_day_key'].nunique()\n",
    "print('Total number of vent days for eval in cohort:', total_vent_days)\n",
    "\n",
    "eligible_days = final_df[final_df['eligible_day'] == 1]['hosp_id_day_key'].nunique()\n",
    "\n",
    "percentage = (eligible_days / total_vent_days) * 100 if total_days > 0 else 0\n",
    "print(f\"Eligible days: {eligible_days} / {total_vent_days} ({percentage:.2f}%)\")\n",
    "print('Hospital days with atleast one IMV event: ',final_df[final_df['device_category'] == 'imv' ]['hosp_id_day_key'].nunique())\n",
    "print('Hospital days with atleast one IMV & ICU event: ',final_df[(final_df['device_category'] == 'imv') &\n",
    "        (final_df['location_category'] == 'icu')]['hosp_id_day_key'].nunique())\n",
    "\n",
    "print('By n = Encounter')\n",
    "h_total_days = final_df['hospitalization_id'].nunique()\n",
    "print('Total number of days for eval in cohort:', h_total_days)\n",
    "h_eligible_days = final_df[final_df['eligible_day'] == 1]['hospitalization_id'].nunique()\n",
    "h_percentage = (h_eligible_days / h_total_days) * 100 if h_total_days > 0 else 0\n",
    "print(f\"Eligible days: {h_eligible_days} / {h_total_days} ({h_percentage:.2f}%)\")\n",
    "print('Hospital days with atleast one IMV event: ',final_df[final_df['device_category'] == 'imv' ]['hospitalization_id'].nunique())\n",
    "print('Hospital days with atleast one IMV & ICU event: ',final_df[(final_df['device_category'] == 'imv') &\n",
    "        (final_df['location_category'] == 'icu')]['hospitalization_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLIP Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diagnostic_flip_sbt_optimized_v2(cohort):\n",
    "    # Ensure event_time is datetime.\n",
    "    cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "    \n",
    "    # Preinitialize diagnostic and flip evaluation columns.\n",
    "    diag_cols = ['cond_device_imv', 'cond_location_icu', 'cond_mode_ps_cpap',\n",
    "                 'cond_ps_set_le8', 'cond_peep_set_le8', 'cond_mode_tpiece',\n",
    "                 'flip_skip_reason', 'first_flip_time']\n",
    "    for col in diag_cols:\n",
    "        cohort[col] = None\n",
    "        \n",
    "    # Initialize EHR delivery columns.\n",
    "    for mins in [2, 30]:\n",
    "        cohort[f\"EHR_Delivery_{mins}mins\"] = pd.NaT\n",
    "\n",
    "    # --- Precompute diagnostic flags (vectorized) ---\n",
    "    mask_eligible = cohort['eligible_day'] == 1\n",
    "    \n",
    "    # Normalize and compare strings.\n",
    "    cond_imv = cohort['device_category'].fillna('').str.strip().str.lower() == 'imv'\n",
    "    cond_icu = cohort['location_category'].fillna('').str.strip().str.lower() == 'icu'\n",
    "    \n",
    "    mode_cat_lower = cohort['mode_category'].fillna('').str.lower()\n",
    "    cond_mode_ps = mode_cat_lower.str.contains('pressure support|cpap', regex=True)\n",
    "    cond_ps_le8 = cohort['pressure_support_set'] <= 8\n",
    "    cond_peep_le8 = cohort['peep_set'] <= 8\n",
    "    conditionA = cond_mode_ps & cond_ps_le8 & cond_peep_le8\n",
    "    mode_name_lower = cohort['mode_name'].fillna('').str.strip().str.lower()\n",
    "    cond_mode_tpiece = mode_name_lower.str.match(r'^t[-]?piece$', na=False)\n",
    "    composite = conditionA | cond_mode_tpiece\n",
    "    passed = cond_imv & cond_icu & composite\n",
    "\n",
    "    # Set diagnostic columns for eligible rows.\n",
    "    cohort.loc[mask_eligible & (~cond_imv), 'cond_device_imv'] = \\\n",
    "        cohort.loc[mask_eligible & (~cond_imv), 'device_category']\n",
    "    cohort.loc[mask_eligible & cond_imv & (~cond_icu), 'cond_location_icu'] = \\\n",
    "        cohort.loc[mask_eligible & cond_imv & (~cond_icu), 'location_category']\n",
    "    \n",
    "    mask_composite_fail = mask_eligible & cond_imv & cond_icu & (~composite)\n",
    "    cohort.loc[mask_composite_fail & (~cond_mode_ps), 'cond_mode_ps_cpap'] = \\\n",
    "        cohort.loc[mask_composite_fail & (~cond_mode_ps), 'mode_category']\n",
    "    mask_ps_fail = cohort['pressure_support_set'].isnull() | (cohort['pressure_support_set'] > 8)\n",
    "    cohort.loc[mask_composite_fail & mask_ps_fail, 'cond_ps_set_le8'] = \\\n",
    "        cohort.loc[mask_composite_fail & mask_ps_fail, 'pressure_support_set']\n",
    "    mask_peep_fail = cohort['peep_set'].isnull() | (cohort['peep_set'] > 8)\n",
    "    cohort.loc[mask_composite_fail & mask_peep_fail, 'cond_peep_set_le8'] = \\\n",
    "        cohort.loc[mask_composite_fail & mask_peep_fail, 'peep_set']\n",
    "    cohort.loc[mask_composite_fail & (~cond_mode_tpiece), 'cond_mode_tpiece'] = \\\n",
    "        cohort.loc[mask_composite_fail & (~cond_mode_tpiece), 'mode_name']\n",
    "    \n",
    "    # Mark candidate rows.\n",
    "    cohort['flip_check_flag'] = False\n",
    "    cohort.loc[mask_eligible, 'flip_check_flag'] = passed[mask_eligible]\n",
    "    \n",
    "    # Compute the minimum IMV_Controlled_met_time per eligible group.\n",
    "    cohort.loc[mask_eligible, 'min_met_time'] = (\n",
    "        cohort.loc[mask_eligible]\n",
    "        .groupby(['hospitalization_id', 'current_day'])['IMV_Controlled_met_time']\n",
    "        .transform('min')\n",
    "    )\n",
    "    \n",
    "    # --- Process each eligible group using vectorized operations ---\n",
    "    def process_group(group):\n",
    "        # Work on a copy sorted by event_time.\n",
    "        group = group.sort_values('event_time').copy()\n",
    "        n = len(group)\n",
    "        if n == 0:\n",
    "            return group\n",
    "        \n",
    "        # Convert event_time to numpy array.\n",
    "        times = group['event_time'].values.astype('datetime64[ns]')\n",
    "        flip_int = group['flip_check_flag'].astype(int).values\n",
    "\n",
    "        def compute_sustained(delta_minutes):\n",
    "            delta = np.timedelta64(delta_minutes, 'm')\n",
    "            boundaries = np.searchsorted(times, times + delta, side='right')\n",
    "            cnt_total = boundaries - np.arange(n)\n",
    "            cumsum = np.cumsum(flip_int)\n",
    "            cnt_pass = np.empty(n, dtype=int)\n",
    "            for i in range(n):\n",
    "                start = i\n",
    "                end = boundaries[i] - 1\n",
    "                cnt_pass[i] = cumsum[end] - (cumsum[start-1] if start > 0 else 0)\n",
    "            return (cnt_total == cnt_pass) & group['flip_check_flag'], cnt_total, cnt_pass\n",
    "\n",
    "        # Compute sustained flags for 2 mins and 30 mins\n",
    "        group['sustained_2min'], group['cnt_total_2'], group['cnt_pass_2'] = compute_sustained(2)\n",
    "        group['sustained_30min'], group['cnt_total_30'], group['cnt_pass_30'] = compute_sustained(30)\n",
    "\n",
    "        # Apply 2-min logic\n",
    "        candidate_indices = group.index[group['flip_check_flag']].tolist()\n",
    "        for idx in candidate_indices:\n",
    "            group.at[idx, 'first_flip_time'] = group.at[idx, 'event_time']\n",
    "            if group.at[idx, 'event_time'] <= group.at[idx, 'min_met_time']:\n",
    "                group.at[idx, 'flip_skip_reason'] = \"Flip before IMV_Controlled_met_time\"\n",
    "                continue\n",
    "            else:\n",
    "                if group.at[idx, 'sustained_2min']:\n",
    "                    group.at[idx, 'EHR_Delivery_2mins'] = 1\n",
    "                    group.at[idx, 'flip_skip_reason'] = None\n",
    "                    break\n",
    "                else:\n",
    "                    group.at[idx, 'flip_skip_reason'] = \"ehr_delivery_2min not possible\"\n",
    "                    continue\n",
    "\n",
    "        # Apply 30-min logic (independently)\n",
    "        for idx in candidate_indices:\n",
    "            if group.at[idx, 'event_time'] <= group.at[idx, 'min_met_time']:\n",
    "                continue\n",
    "            if group.at[idx, 'sustained_30min']:\n",
    "                group.at[idx, 'EHR_Delivery_30mins'] = 1\n",
    "                break\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply the per-group processing only on eligible rows.\n",
    "    eligible_df = cohort[mask_eligible].copy()\n",
    "    processed = eligible_df.groupby(['hospitalization_id', 'current_day'], group_keys=False).apply(process_group)\n",
    "    \n",
    "    # Update only the eligible rows in the original DataFrame.\n",
    "    cohort.update(processed)\n",
    "    \n",
    "    # Remove helper columns.\n",
    "    helper_cols = ['cnt_total_2', 'cnt_pass_2', 'sustained_2min',\n",
    "                   'cnt_total_30', 'cnt_pass_30', 'sustained_30min', 'min_met_time']\n",
    "    cohort.drop(columns=[col for col in helper_cols if col in cohort.columns], inplace=True)\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "final_df = process_diagnostic_flip_sbt_optimized_v2(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_2_45_extubated_flag(cohort):\n",
    "    # Ensure time columns are datetime\n",
    "    cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "    cohort['first_flip_time'] = pd.to_datetime(cohort['first_flip_time'])\n",
    "\n",
    "    # Initialize flag column\n",
    "    cohort['flag_2_45_extubated'] = np.nan\n",
    "\n",
    "    # Loop over each group\n",
    "    group_cols = ['hospitalization_id', 'current_day']\n",
    "    for (hosp_id, day), group in cohort.groupby(group_cols):\n",
    "        flip_row = group[(group['EHR_Delivery_2mins'] == 1) & (~group['first_flip_time'].isna())]\n",
    "        if flip_row.empty:\n",
    "            continue\n",
    "\n",
    "        flip_time = flip_row.iloc[0]['first_flip_time']\n",
    "        time_window_end = flip_time + pd.Timedelta(minutes=45)\n",
    "\n",
    "        # Look for extubation within time window\n",
    "        extubation_mask = (group['event_time'] > flip_time) & \\\n",
    "                          (group['event_time'] <= time_window_end) & \\\n",
    "                          (group['extubated'] == 1)\n",
    "\n",
    "        if extubation_mask.any():\n",
    "            cohort.loc[flip_row.index[0], 'flag_2_45_extubated'] = 1\n",
    "\n",
    "    return cohort\n",
    "\n",
    "final_df = apply_2_45_extubated_flag(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_to_extubation(cohort):\n",
    "    # Ensure time columns are datetime\n",
    "    cohort['event_time'] = pd.to_datetime(cohort['event_time'])\n",
    "    cohort['first_flip_time'] = pd.to_datetime(cohort['first_flip_time'])\n",
    "\n",
    "    # Initialize new column\n",
    "    cohort['delta_to_extubation_mins'] = np.nan\n",
    "\n",
    "    # Grouping by patient and day\n",
    "    group_cols = ['hospitalization_id', 'current_day']\n",
    "    for (hosp_id, day), group in cohort.groupby(group_cols):\n",
    "        group = group.sort_values('event_time')\n",
    "\n",
    "        flip_row = group[(group['EHR_Delivery_30mins'] == 1) & (~group['first_flip_time'].isna())]\n",
    "        if flip_row.empty:\n",
    "            continue\n",
    "\n",
    "        flip_time = flip_row.iloc[0]['first_flip_time']\n",
    "        flip_index = flip_row.index[0]\n",
    "\n",
    "        # Find first extubation event *after* flip_time\n",
    "        post_extubated = group[(group['event_time'] > flip_time) & (group['extubated'] == 1)]\n",
    "        if not post_extubated.empty:\n",
    "            extubation_time = post_extubated.iloc[0]['event_time']\n",
    "            delta = (extubation_time - flip_time).total_seconds() / 60.0\n",
    "            cohort.loc[flip_index, 'delta_to_extubation_mins'] = delta\n",
    "\n",
    "    return cohort\n",
    "\n",
    "final_df = compute_time_to_extubation(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA hospital_ids and get unique ones\n",
    "hospital_ids = final_df['hospital_id'].dropna().unique()\n",
    "\n",
    "# Define hourly bins (0–1440 mins, i.e., 24 hrs) and labels\n",
    "bins = list(range(0, 24 * 60 + 1, 60))  # 0 to 1440 mins in 60-min intervals\n",
    "labels = [f'{i}-{i+1}hr' for i in range(24)]  # '0-1hr', '1-2hr', ..., '23-24hr'\n",
    "\n",
    "# List to store per-hospital rush count rows\n",
    "rush_summary = []\n",
    "\n",
    "# Loop over each hospital\n",
    "for hosp in hospital_ids:\n",
    "    # Filter and bin delta times\n",
    "    delta_series = final_df[final_df['hospital_id'] == hosp]['delta_to_extubation_mins'].dropna()\n",
    "    delta_binned = pd.cut(delta_series, bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Count entries per bin and convert to dictionary\n",
    "    rush_counts = delta_binned.value_counts().sort_index()\n",
    "    rush_counts_dict = rush_counts.to_dict()\n",
    "\n",
    "    # Add hospital_id to the result row\n",
    "    rush_counts_dict['hospital_id'] = hosp\n",
    "\n",
    "    # Append to summary list\n",
    "    rush_summary.append(rush_counts_dict)\n",
    "\n",
    "    pd.DataFrame(final_df[final_df['hospital_id'] == hosp]['delta_to_extubation_mins'].describe()).to_csv(f\"{directory_path}/delta_stats_between_EHR30Min_Extubated_{hosp}.csv\")\n",
    "\n",
    "# Convert all to a DataFrame\n",
    "rush_df = pd.DataFrame(rush_summary)\n",
    "\n",
    "# Fill missing bins (if any hospital didn’t have extubations in certain bins)\n",
    "rush_df = rush_df.fillna(0).astype({col: 'int' for col in rush_df.columns if col != 'hospital_id'})\n",
    "\n",
    "# Save to CSV\n",
    "rush_df.to_csv(f\"{directory_path}/rush_counts_by_hour_per_hospital.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_series = final_df.delta_to_extubation_mins.dropna()\n",
    "# Create bins for each hour till 24 hours\n",
    "bins = list(range(0, 24*60 + 1, 60))  # from 0 to 1440 minutes (24 hrs) with 60-min intervals\n",
    "labels = [f'{i}-{i+1}hr' for i in range(24)]  # Label bins as '0-1hr', '1-2hr', ..., '23-24hr'\n",
    "delta_binned = pd.cut(delta_series, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the number of entries in each bin\n",
    "rush_counts = delta_binned.value_counts().sort_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rush_counts.index, rush_counts.values, marker='o')\n",
    "plt.title('Overall: Count of Extubation Events per Hour Bin after EHR signature (30 mins)')\n",
    "plt.xlabel('Hours since event (binned)')\n",
    "plt.ylabel('Number of Extubations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['sbt_bkp'] = final_df['sbt_delivery_pass_fail']\n",
    "final_df['sbt_delivery_pass_fail'] = final_df['sbt_delivery_pass_fail'].map({0:1,1:1})\n",
    "final_df['sbt_screen_pass_fail'] = final_df['sbt_screen_pass_fail'].map({0:1,1:1})\n",
    "\n",
    "#fill forward reason of flip fail\n",
    "final_df['flip_skip_reason'] = (\n",
    "    final_df.groupby('hosp_id_day_key')['flip_skip_reason']\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the specified columns are treated as datetime before calculating percentages\n",
    "datetime_columns = [\n",
    "    'EHR_Delivery_2mins',\n",
    "    'EHR_Delivery_30mins'\n",
    "]\n",
    "\n",
    "for col in datetime_columns:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and aggregate the DataFrame including the extubation check\n",
    "grouped_df = final_df.groupby('hosp_id_day_key').agg({\n",
    "    'hospitalization_id' : 'first',\n",
    "    'hospital_id': lambda x: x.dropna().iloc[-1] if x.dropna().size > 0 else np.nan,\n",
    "    'eligible_day': 'max',\n",
    "    'EHR_Delivery_2mins': 'max',\n",
    "    'EHR_Delivery_30mins': 'max',\n",
    "    'sat_screen_pass_fail': 'max',\n",
    "    'sat_delivery_pass_fail': 'max',\n",
    "    'sbt_screen_pass_fail': 'max',\n",
    "    'sbt_delivery_pass_fail': 'max',\n",
    "    'flag_2_45_extubated': 'max',  # Uncomment if needed\n",
    "    'flip_skip_reason': lambda x: x.dropna().iloc[-1] if x.dropna().size > 0 else np.nan,\n",
    "    'extubated': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the aggregated device_category column to extubated and fill NaN values\n",
    "mat_df = grouped_df[grouped_df['eligible_day']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA hospital_ids and get unique ones\n",
    "hospital_ids = grouped_df['hospital_id'].dropna().unique()\n",
    "\n",
    "# Container for summary rows\n",
    "summary_data = []\n",
    "\n",
    "# Loop over hospitals and compute stats\n",
    "for hosp in hospital_ids:\n",
    "    # Filter data for the hospital and eligible days\n",
    "    df_hosp = grouped_df[grouped_df['hospital_id'] == hosp]\n",
    "    df_eligible = df_hosp[df_hosp['eligible_day'] == 1]\n",
    "\n",
    "    # Calculate condition-specific sets (unique hosp_id_day_keys)\n",
    "    sbt_S = set(df_eligible[df_eligible['sbt_screen_pass_fail'] == 1]['hosp_id_day_key'].unique())\n",
    "    sbt_D = set(df_eligible[df_eligible['sbt_delivery_pass_fail'] == 1]['hosp_id_day_key'].unique())\n",
    "    ehr_2min = set(df_eligible[df_eligible['EHR_Delivery_2mins'] == 1]['hosp_id_day_key'].unique())\n",
    "    ehr_30min = set(df_eligible[df_eligible['EHR_Delivery_30mins'] == 1]['hosp_id_day_key'].unique())\n",
    "    ehr_extubated = set(df_eligible[df_eligible['extubated'] == 1]['hosp_id_day_key'].unique())\n",
    "    ehr_2min_45min_extubated = set(df_eligible[df_eligible['flag_2_45_extubated'] == 1]['hosp_id_day_key'].unique())\n",
    "\n",
    "    # Append aggregated counts to summary list\n",
    "    summary_data.append({\n",
    "        'hospital_id': hosp,\n",
    "        'sbt_screen_pass': len(sbt_S),\n",
    "        'sbt_delivery_pass': len(sbt_D),\n",
    "        'ehr_2min': len(ehr_2min),\n",
    "        'ehr_30min': len(ehr_30min),\n",
    "        'extubated': len(ehr_extubated),\n",
    "        'ehr_2min_45min_extubated': len(ehr_2min_45min_extubated)\n",
    "    })\n",
    "\n",
    "    # Optionally print the stats\n",
    "    print(f\"\\nHospital ID: {hosp}\")\n",
    "    print(f\"  SBT Screen Pass: {len(sbt_S)}\")\n",
    "    print(f\"  SBT Delivery Pass: {len(sbt_D)}\")\n",
    "    print(f\"  EHR 2-min Delivery: {len(ehr_2min)}\")\n",
    "    print(f\"  EHR 30-min Delivery: {len(ehr_30min)}\")\n",
    "    print(f\"  Extubated: {len(ehr_extubated)}\")\n",
    "    print(f\"  ehr_2min_45min_extubated: {len(ehr_2min_45min_extubated)}\")\n",
    "\n",
    "# Convert summary list to DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df\n",
    "\n",
    "summary_df.to_csv(f\"{directory_path}/hospital_sbt_ehr_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vs SBT flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "mat_df['sbt_delivery_pass_fail'] = mat_df['sbt_delivery_pass_fail'].fillna(0)\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # Filter the DataFrame for the current hospital\n",
    "    df_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Create the confusion matrix using pd.crosstab\n",
    "    conf_matrix = pd.crosstab(df_hosp['EHR_Delivery_2mins'], df_hosp['sbt_delivery_pass_fail'])\n",
    "    \n",
    "    # Calculate percentages for each cell\n",
    "    conf_matrix_percent = conf_matrix / conf_matrix.values.sum() * 100\n",
    "    \n",
    "    # Create annotation labels that combine count and percentage\n",
    "    annot = conf_matrix.astype(str) + \"\\n\" + conf_matrix_percent.round(1).astype(str) + \"%\"\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=annot, fmt='', cmap=\"Blues\", xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.xlabel(\"SBT Delivery in Flowsheet\")\n",
    "    plt.ylabel(\"EHR Delivery in 2 minutes\")\n",
    "    plt.title(f\"Confusion Matrix for Hospital {hosp}\")\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f\"{directory_path}/confusion_matrix_{hosp}_by_SBT.png\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    \n",
    "    # Extract ground truth and predictions for the current hospital\n",
    "    y_true = df_hosp['EHR_Delivery_2mins']\n",
    "    y_pred = df_hosp['sbt_delivery_pass_fail']\n",
    "    \n",
    "    # Compute the confusion matrix and extract TP, FP, FN, TN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate individual metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    # Print metrics for current hospital (optional)\n",
    "    print(f\"Hospital ID: {hosp}\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "    \n",
    "    # Create a dictionary with the computed metrics\n",
    "    metrics_dict = {\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "\n",
    "    # Build a DataFrame to store the metrics\n",
    "    df_metrics = pd.DataFrame(list(metrics_dict.items()), columns=[\"Metric\", \"Value\"])\n",
    "    \n",
    "    # Save the metrics DataFrame as a CSV file\n",
    "    df_metrics.to_csv(f\"{directory_path}/EHR_vs_SBT_metrics_{hosp}.csv\", index=False)\n",
    "    print(hosp,df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vs extubated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "mat_df['extubated'] =mat_df['extubated'].fillna(0)\n",
    "for hosp in hospital_ids:\n",
    "    # Filter the DataFrame for the current hospital\n",
    "    df_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Create the confusion matrix using pd.crosstab\n",
    "    conf_matrix = pd.crosstab(df_hosp['EHR_Delivery_2mins'], df_hosp['extubated'])\n",
    "    \n",
    "    # Calculate percentages for each cell\n",
    "    conf_matrix_percent = conf_matrix / conf_matrix.values.sum() * 100\n",
    "    \n",
    "    # Create annotation labels that combine count and percentage\n",
    "    annot = conf_matrix.astype(str) + \"\\n\" + conf_matrix_percent.round(1).astype(str) + \"%\"\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=annot, fmt='', cmap=\"Blues\", xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.xlabel(\"extubated\")\n",
    "    plt.ylabel(\"EHR Delivery in 2 minutes\")\n",
    "    plt.title(f\"Confusion Matrix for Hospital {hosp}\")\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f\"{directory_path}/confusion_matrix_{hosp}_by_extubated.png\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    \n",
    "    # Extract ground truth and predictions for the current hospital\n",
    "    y_true = df_hosp['EHR_Delivery_2mins']\n",
    "    y_pred = df_hosp['extubated']\n",
    "    \n",
    "    # Compute the confusion matrix and extract TP, FP, FN, TN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate individual metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    # Print metrics for current hospital (optional)\n",
    "    print(f\"Hospital ID: {hosp}\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "    \n",
    "    # Create a dictionary with the computed metrics\n",
    "    metrics_dict = {\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "    \n",
    "    # Build a DataFrame to store the metrics\n",
    "    df_metrics = pd.DataFrame(list(metrics_dict.items()), columns=[\"Metric\", \"Value\"])\n",
    "    \n",
    "    # Save the metrics DataFrame as a CSV file\n",
    "    df_metrics.to_csv(f\"{directory_path}/EHR_VS_EXTUBATED_metrics_{hosp}.csv\", index=False)\n",
    "    print(hosp,df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EHR 30 vs Extubated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "mat_df['extubated'] =mat_df['extubated'].fillna(0)\n",
    "for hosp in hospital_ids:\n",
    "    # Filter the DataFrame for the current hospital\n",
    "    df_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Create the confusion matrix using pd.crosstab\n",
    "    conf_matrix = pd.crosstab(df_hosp['EHR_Delivery_30mins'], df_hosp['extubated'])\n",
    "    \n",
    "    # Calculate percentages for each cell\n",
    "    conf_matrix_percent = conf_matrix / conf_matrix.values.sum() * 100\n",
    "    \n",
    "    # Create annotation labels that combine count and percentage\n",
    "    annot = conf_matrix.astype(str) + \"\\n\" + conf_matrix_percent.round(1).astype(str) + \"%\"\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=annot, fmt='', cmap=\"Blues\", xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.xlabel(\"extubated\")\n",
    "    plt.ylabel(\"EHR Delivery in 30 minutes\")\n",
    "    plt.title(f\"Confusion Matrix for Hospital {hosp}\")\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f\"{directory_path}/ehr_30_confusion_matrix_{hosp}_by_extubated.png\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    \n",
    "    # Extract ground truth and predictions for the current hospital\n",
    "    y_true = df_hosp['EHR_Delivery_30mins']\n",
    "    y_pred = df_hosp['extubated']\n",
    "    \n",
    "    # Compute the confusion matrix and extract TP, FP, FN, TN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate individual metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    # Print metrics for current hospital (optional)\n",
    "    print(f\"Hospital ID: {hosp}\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "    \n",
    "    # Create a dictionary with the computed metrics\n",
    "    metrics_dict = {\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "    \n",
    "    # Build a DataFrame to store the metrics\n",
    "    df_metrics = pd.DataFrame(list(metrics_dict.items()), columns=[\"Metric\", \"Value\"])\n",
    "    \n",
    "    # Save the metrics DataFrame as a CSV file\n",
    "    df_metrics.to_csv(f\"{directory_path}/EHR_30_VS_EXTUBATED_metrics_{hosp}.csv\", index=False)\n",
    "    print(hosp,df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EHR 30 VS SBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "mat_df['sbt_delivery_pass_fail'] = mat_df['sbt_delivery_pass_fail'].fillna(0)\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # Filter the DataFrame for the current hospital\n",
    "    df_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Create the confusion matrix using pd.crosstab\n",
    "    conf_matrix = pd.crosstab(df_hosp['EHR_Delivery_30mins'], df_hosp['sbt_delivery_pass_fail'])\n",
    "    \n",
    "    # Calculate percentages for each cell\n",
    "    conf_matrix_percent = conf_matrix / conf_matrix.values.sum() * 100\n",
    "    \n",
    "    # Create annotation labels that combine count and percentage\n",
    "    annot = conf_matrix.astype(str) + \"\\n\" + conf_matrix_percent.round(1).astype(str) + \"%\"\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=annot, fmt='', cmap=\"Blues\", xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.xlabel(\"SBT Delivery in Flowsheet\")\n",
    "    plt.ylabel(\"EHR Delivery in 30 minutes\")\n",
    "    plt.title(f\"Confusion Matrix for Hospital {hosp}\")\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f\"{directory_path}/ehr_30_confusion_matrix_{hosp}_by_SBT.png\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    \n",
    "    # Extract ground truth and predictions for the current hospital\n",
    "    y_true = df_hosp['EHR_Delivery_30mins']\n",
    "    y_pred = df_hosp['sbt_delivery_pass_fail']\n",
    "    \n",
    "    # Compute the confusion matrix and extract TP, FP, FN, TN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate individual metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    \n",
    "    # Print metrics for current hospital (optional)\n",
    "    print(f\"Hospital ID: {hosp}\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "    \n",
    "    # Create a dictionary with the computed metrics\n",
    "    metrics_dict = {\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "\n",
    "    # Build a DataFrame to store the metrics\n",
    "    df_metrics = pd.DataFrame(list(metrics_dict.items()), columns=[\"Metric\", \"Value\"])\n",
    "    \n",
    "    # Save the metrics DataFrame as a CSV file\n",
    "    df_metrics.to_csv(f\"{directory_path}/EHR_30_vs_SBT_metrics_{hosp}.csv\", index=False)\n",
    "    print(hosp,df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fail compared to SBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # -------------------------------\n",
    "    # Filter the data for the current hospital\n",
    "    # -------------------------------\n",
    "    mat_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Step 1: Extract filtered keys from mat_hosp\n",
    "    # -------------------------------\n",
    "    filtered_keys = mat_hosp.loc[\n",
    "        (mat_hosp['EHR_Delivery_2mins'] == 0) & (mat_hosp['sbt_delivery_pass_fail'] == 1),\n",
    "        'hosp_id_day_key'\n",
    "    ].unique()\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Step 2: Filter final_hosp using these keys\n",
    "    # -------------------------------\n",
    "    final_filtered_df = final_df.loc[\n",
    "        (final_df['sbt_delivery_pass_fail'] == 1) & \n",
    "        (final_df['hosp_id_day_key'].isin(filtered_keys))\n",
    "    ]\n",
    "    \n",
    "    final_filtered_df = final_filtered_df.sort_values('event_time')\n",
    "    final_filtered_df = final_filtered_df.drop_duplicates(subset='hosp_id_day_key', keep='first')\n",
    "    \n",
    "    print(f\"Hospital: {hosp}, final_filtered_df shape: {final_filtered_df.shape}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Work on a copy for filtering steps\n",
    "    # -------------------------------\n",
    "    df = final_filtered_df.copy()\n",
    "    results = []\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 1: Filter on 'flip_skip_reason'\n",
    "    # ---------------------------------------\n",
    "    step1 = df[~df['flip_skip_reason'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 1',\n",
    "        'FilterColumn': 'flip_skip_reason',\n",
    "        'UniqueKeys': step1['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step1.shape[0],\n",
    "        'ValueCounts': step1['flip_skip_reason'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step1['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 2: Filter on 'cond_device_imv'\n",
    "    # ---------------------------------------\n",
    "    step2 = df[~df['cond_device_imv'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 2',\n",
    "        'FilterColumn': 'cond_device_imv',\n",
    "        'UniqueKeys': step2['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step2.shape[0],\n",
    "        'ValueCounts': step2['cond_device_imv'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step2['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 3: Filter on 'cond_location_icu'\n",
    "    # ---------------------------------------\n",
    "    step3 = df[~df['cond_location_icu'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 3',\n",
    "        'FilterColumn': 'cond_location_icu',\n",
    "        'UniqueKeys': step3['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step3.shape[0],\n",
    "        'ValueCounts': step3['cond_location_icu'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step3['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 4: Filter on 'cond_peep_set_le8'\n",
    "    # ---------------------------------------\n",
    "    step4 = df[~df['cond_peep_set_le8'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 4',\n",
    "        'FilterColumn': 'cond_peep_set_le8',\n",
    "        'UniqueKeys': step4['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step4.shape[0],\n",
    "        'ValueCounts': step4['cond_peep_set_le8'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step4['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 5: Filter on 'cond_ps_set_le8'\n",
    "    # ---------------------------------------\n",
    "    step5 = df[~df['cond_ps_set_le8'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 5',\n",
    "        'FilterColumn': 'cond_ps_set_le8',\n",
    "        'UniqueKeys': step5['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step5.shape[0],\n",
    "        'ValueCounts': step5['cond_ps_set_le8'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step5['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 6: Filter on 'cond_mode_ps_cpap'\n",
    "    # ---------------------------------------\n",
    "    step6 = df[~df['cond_mode_ps_cpap'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 6',\n",
    "        'FilterColumn': 'cond_mode_ps_cpap',\n",
    "        'UniqueKeys': step6['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step6.shape[0],\n",
    "        'ValueCounts': step6['cond_mode_ps_cpap'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step6['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 7: Remaining (unmatched) rows\n",
    "    # ---------------------------------------\n",
    "    step7 = df.copy()\n",
    "    results.append({\n",
    "        'Step': 'Step 7 (Unmatched)',\n",
    "        'FilterColumn': None,\n",
    "        'UniqueKeys': step7['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step7.shape[0],\n",
    "        'ValueCounts': None\n",
    "    })\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Create Detailed Step-by-Step Summary DataFrame\n",
    "    # ---------------------------------------\n",
    "    detailed_summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate total_failures as the sum of UniqueKeys across all steps for this hospital\n",
    "    total_failures = detailed_summary_df['UniqueKeys'].sum()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Add \"% Per 100\" and \"% of Total\" columns\n",
    "    detailed_summary_df['% by eligible_days'] = detailed_summary_df['UniqueKeys'].apply(\n",
    "        lambda x: round((x / eligible_days) * 100, 2)\n",
    "    )\n",
    "    detailed_summary_df['% of Total'] = detailed_summary_df['UniqueKeys'].apply(\n",
    "        lambda x: round((x / total_failures) * 100, 2) if total_failures != 0 else 0\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Save the detailed summary DataFrame as a CSV file for the current hospital\n",
    "    # ---------------------------------------\n",
    "    output_filename = f\"{directory_path}/EHR_VS_SBT_failure_dependent_summary_{hosp}.csv\"\n",
    "    detailed_summary_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved detailed summary for hospital {hosp} to {output_filename}\\n\")\n",
    "    print(hosp,detailed_summary_df)\n",
    "    print()\n",
    "\n",
    "    # ============================================================\n",
    "    # B. Independent Filtering Summary (Apply each filter independently)\n",
    "    # ============================================================\n",
    "    ind_step1 = final_filtered_df[~final_filtered_df['flip_skip_reason'].isna()]\n",
    "    ind_step2 = final_filtered_df[~final_filtered_df['cond_device_imv'].isna()]\n",
    "    ind_step3 = final_filtered_df[~final_filtered_df['cond_location_icu'].isna()]\n",
    "    ind_step4 = final_filtered_df[~final_filtered_df['cond_peep_set_le8'].isna()]\n",
    "    ind_step5 = final_filtered_df[~final_filtered_df['cond_ps_set_le8'].isna()]\n",
    "    ind_step6 = final_filtered_df[~final_filtered_df['cond_mode_ps_cpap'].isna()]\n",
    "    \n",
    "    # Determine the union of keys matched by any filter\n",
    "    matched_keys = set().union(\n",
    "        ind_step1['hosp_id_day_key'],\n",
    "        ind_step2['hosp_id_day_key'],\n",
    "        ind_step3['hosp_id_day_key'],\n",
    "        ind_step4['hosp_id_day_key'],\n",
    "        ind_step5['hosp_id_day_key'],\n",
    "        ind_step6['hosp_id_day_key']\n",
    "    )\n",
    "    # Unmatched keys: those not included in any of the independent filters\n",
    "    ind_step7 = final_filtered_df[~final_filtered_df['hosp_id_day_key'].isin(matched_keys)]\n",
    "    \n",
    "    # Compute unique key counts per filter\n",
    "    failure_counts = {\n",
    "        'flip_skip_reason': ind_step1['hosp_id_day_key'].nunique(),\n",
    "        'cond_device_imv': ind_step2['hosp_id_day_key'].nunique(),\n",
    "        'cond_location_icu': ind_step3['hosp_id_day_key'].nunique(),\n",
    "        'cond_peep_set_le8': ind_step4['hosp_id_day_key'].nunique(),\n",
    "        'cond_ps_set_le8': ind_step5['hosp_id_day_key'].nunique(),\n",
    "        'cond_mode_ps_cpap': ind_step6['hosp_id_day_key'].nunique(),\n",
    "        'unmatched': ind_step7['hosp_id_day_key'].nunique()\n",
    "    }\n",
    "    \n",
    "    # Compute value counts for each filter column\n",
    "    value_counts_map = {\n",
    "        'flip_skip_reason': ind_step1['flip_skip_reason'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_device_imv': ind_step2['cond_device_imv'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_location_icu': ind_step3['cond_location_icu'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_peep_set_le8': ind_step4['cond_peep_set_le8'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_ps_set_le8': ind_step5['cond_ps_set_le8'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_mode_ps_cpap': ind_step6['cond_mode_ps_cpap'].value_counts(dropna=False).to_dict(),\n",
    "        'unmatched': None\n",
    "    }\n",
    "    \n",
    "    total_failures_ind = sum(failure_counts.values())\n",
    "    summary_data = []\n",
    "    for reason, count in failure_counts.items():\n",
    "        summary_data.append({\n",
    "            'Failure Reason': reason,\n",
    "            'Count': count,\n",
    "            '% by eligible_days': round((count / eligible_days) * 100, 2),\n",
    "            '% of Total (out of total failed cases)': round((count / total_failures_ind) * 100, 2) if total_failures_ind else 0,\n",
    "            'Value Counts': value_counts_map[reason]\n",
    "        })\n",
    "    \n",
    "    independent_summary_df = pd.DataFrame(summary_data)\n",
    "    independent_summary_df = independent_summary_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ind_output_filename = f\"{directory_path}/EHR_VS_SBT_failure_independent_summary_hospital_{hosp}.csv\"\n",
    "    independent_summary_df.to_csv(ind_output_filename, index=False)\n",
    "    print(f\"Saved independent summary for hospital {hosp} to {ind_output_filename}\\n\")\n",
    "    print(hosp, independent_summary_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure compared to extubated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = mat_df['hospital_id'].unique()\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # -------------------------------\n",
    "    # Filter the data for the current hospital\n",
    "    # -------------------------------\n",
    "    mat_hosp = mat_df[mat_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Step 1: Extract filtered keys from mat_hosp\n",
    "    # -------------------------------\n",
    "    filtered_keys = mat_hosp.loc[\n",
    "        (mat_hosp['EHR_Delivery_2mins'] == 0) & (mat_hosp['extubated'] == 1),\n",
    "        'hosp_id_day_key'\n",
    "    ].unique()\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Step 2: Filter final_hosp using these keys\n",
    "    # -------------------------------\n",
    "    final_filtered_df = final_df.loc[\n",
    "        (final_df['extubated'] == 1) & \n",
    "        (final_df['hosp_id_day_key'].isin(filtered_keys))\n",
    "    ]\n",
    "    \n",
    "    final_filtered_df = final_filtered_df.sort_values('event_time')\n",
    "    final_filtered_df = final_filtered_df.drop_duplicates(subset='hosp_id_day_key', keep='first')\n",
    "    \n",
    "    print(f\"Hospital: {hosp}, final_filtered_df shape: {final_filtered_df.shape}\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Work on a copy for filtering steps\n",
    "    # -------------------------------\n",
    "    df = final_filtered_df.copy()\n",
    "    results = []\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 1: Filter on 'flip_skip_reason'\n",
    "    # ---------------------------------------\n",
    "    step1 = df[~df['flip_skip_reason'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 1',\n",
    "        'FilterColumn': 'flip_skip_reason',\n",
    "        'UniqueKeys': step1['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step1.shape[0],\n",
    "        'ValueCounts': step1['flip_skip_reason'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step1['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 2: Filter on 'cond_device_imv'\n",
    "    # ---------------------------------------\n",
    "    step2 = df[~df['cond_device_imv'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 2',\n",
    "        'FilterColumn': 'cond_device_imv',\n",
    "        'UniqueKeys': step2['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step2.shape[0],\n",
    "        'ValueCounts': step2['cond_device_imv'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step2['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 3: Filter on 'cond_location_icu'\n",
    "    # ---------------------------------------\n",
    "    step3 = df[~df['cond_location_icu'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 3',\n",
    "        'FilterColumn': 'cond_location_icu',\n",
    "        'UniqueKeys': step3['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step3.shape[0],\n",
    "        'ValueCounts': step3['cond_location_icu'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step3['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 4: Filter on 'cond_peep_set_le8'\n",
    "    # ---------------------------------------\n",
    "    step4 = df[~df['cond_peep_set_le8'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 4',\n",
    "        'FilterColumn': 'cond_peep_set_le8',\n",
    "        'UniqueKeys': step4['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step4.shape[0],\n",
    "        'ValueCounts': step4['cond_peep_set_le8'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step4['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 5: Filter on 'cond_ps_set_le8'\n",
    "    # ---------------------------------------\n",
    "    step5 = df[~df['cond_ps_set_le8'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 5',\n",
    "        'FilterColumn': 'cond_ps_set_le8',\n",
    "        'UniqueKeys': step5['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step5.shape[0],\n",
    "        'ValueCounts': step5['cond_ps_set_le8'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step5['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 6: Filter on 'cond_mode_ps_cpap'\n",
    "    # ---------------------------------------\n",
    "    step6 = df[~df['cond_mode_ps_cpap'].isna()]\n",
    "    results.append({\n",
    "        'Step': 'Step 6',\n",
    "        'FilterColumn': 'cond_mode_ps_cpap',\n",
    "        'UniqueKeys': step6['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step6.shape[0],\n",
    "        'ValueCounts': step6['cond_mode_ps_cpap'].value_counts(dropna=False).to_dict()\n",
    "    })\n",
    "    df = df[~df['hosp_id_day_key'].isin(step6['hosp_id_day_key'])]\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Step 7: Remaining (unmatched) rows\n",
    "    # ---------------------------------------\n",
    "    step7 = df.copy()\n",
    "    results.append({\n",
    "        'Step': 'Step 7 (No Value)',\n",
    "        'FilterColumn': None,\n",
    "        'UniqueKeys': step7['hosp_id_day_key'].nunique(),\n",
    "        'RowCount': step7.shape[0],\n",
    "        'ValueCounts': None\n",
    "    })\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Create Detailed Step-by-Step Summary DataFrame\n",
    "    # ---------------------------------------\n",
    "    detailed_summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate total_failures as the sum of UniqueKeys across all steps for this hospital\n",
    "    total_failures = detailed_summary_df['UniqueKeys'].sum()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Add \"% Per 100\" and \"% of Total\" columns\n",
    "    detailed_summary_df['% by eligible_days'] = detailed_summary_df['UniqueKeys'].apply(\n",
    "        lambda x: round((x / eligible_days) * 100, 2)\n",
    "    )\n",
    "    detailed_summary_df['% of Total'] = detailed_summary_df['UniqueKeys'].apply(\n",
    "        lambda x: round((x / total_failures) * 100, 2) if total_failures != 0 else 0\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Save the detailed summary DataFrame as a CSV file for the current hospital\n",
    "    # ---------------------------------------\n",
    "    output_filename = f\"{directory_path}/EHR_VS_EXTUBATED_failure_dependent_summary_{hosp}.csv\"\n",
    "    detailed_summary_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved detailed summary for hospital {hosp} to {output_filename}\\n\")\n",
    "    print(hosp,detailed_summary_df)\n",
    "    print()\n",
    "\n",
    "    # ============================================================\n",
    "    # B. Independent Filtering Summary (Apply each filter independently)\n",
    "    # ============================================================\n",
    "    ind_step1 = final_filtered_df[~final_filtered_df['flip_skip_reason'].isna()]\n",
    "    ind_step2 = final_filtered_df[~final_filtered_df['cond_device_imv'].isna()]\n",
    "    ind_step3 = final_filtered_df[~final_filtered_df['cond_location_icu'].isna()]\n",
    "    ind_step4 = final_filtered_df[~final_filtered_df['cond_peep_set_le8'].isna()]\n",
    "    ind_step5 = final_filtered_df[~final_filtered_df['cond_ps_set_le8'].isna()]\n",
    "    ind_step6 = final_filtered_df[~final_filtered_df['cond_mode_ps_cpap'].isna()]\n",
    "    \n",
    "    # Determine the union of keys matched by any filter\n",
    "    matched_keys = set().union(\n",
    "        ind_step1['hosp_id_day_key'],\n",
    "        ind_step2['hosp_id_day_key'],\n",
    "        ind_step3['hosp_id_day_key'],\n",
    "        ind_step4['hosp_id_day_key'],\n",
    "        ind_step5['hosp_id_day_key'],\n",
    "        ind_step6['hosp_id_day_key']\n",
    "    )\n",
    "    # Unmatched keys: those not included in any of the independent filters\n",
    "    ind_step7 = final_filtered_df[~final_filtered_df['hosp_id_day_key'].isin(matched_keys)]\n",
    "    \n",
    "    # Compute unique key counts per filter\n",
    "    failure_counts = {\n",
    "        'flip_skip_reason': ind_step1['hosp_id_day_key'].nunique(),\n",
    "        'cond_device_imv': ind_step2['hosp_id_day_key'].nunique(),\n",
    "        'cond_location_icu': ind_step3['hosp_id_day_key'].nunique(),\n",
    "        'cond_peep_set_le8': ind_step4['hosp_id_day_key'].nunique(),\n",
    "        'cond_ps_set_le8': ind_step5['hosp_id_day_key'].nunique(),\n",
    "        'cond_mode_ps_cpap': ind_step6['hosp_id_day_key'].nunique(),\n",
    "        'No Value': ind_step7['hosp_id_day_key'].nunique()\n",
    "    }\n",
    "    \n",
    "    # Compute value counts for each filter column\n",
    "    value_counts_map = {\n",
    "        'flip_skip_reason': ind_step1['flip_skip_reason'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_device_imv': ind_step2['cond_device_imv'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_location_icu': ind_step3['cond_location_icu'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_peep_set_le8': ind_step4['cond_peep_set_le8'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_ps_set_le8': ind_step5['cond_ps_set_le8'].value_counts(dropna=False).to_dict(),\n",
    "        'cond_mode_ps_cpap': ind_step6['cond_mode_ps_cpap'].value_counts(dropna=False).to_dict(),\n",
    "        'No Value': None\n",
    "    }\n",
    "    \n",
    "    total_failures_ind = sum(failure_counts.values())\n",
    "    summary_data = []\n",
    "    for reason, count in failure_counts.items():\n",
    "        summary_data.append({\n",
    "            'Failure Reason': reason,\n",
    "            'Count': count,\n",
    "            '% by eligible_days': round((count / eligible_days) * 100, 2),\n",
    "            '% of Total (out of total failed cases)': round((count / total_failures_ind) * 100, 2) if total_failures_ind else 0,\n",
    "            'Value Counts': value_counts_map[reason]\n",
    "        })\n",
    "    \n",
    "    independent_summary_df = pd.DataFrame(summary_data)\n",
    "    independent_summary_df = independent_summary_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ind_output_filename = f\"{directory_path}/EHR_VS_EXTUBATED_failure_independent_summary_{hosp}.csv\"\n",
    "    independent_summary_df.to_csv(ind_output_filename, index=False)\n",
    "    print(f\"Saved independent summary for hospital {hosp} to {ind_output_filename}\\n\")\n",
    "    print(hosp, independent_summary_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = final_df['hospital_id'].dropna().unique()\n",
    "\n",
    "# This list will hold the summary data for each hospital\n",
    "hospital_summary_list = []\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # Filter final_df for the current hospital\n",
    "    final_hosp = final_df[final_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Extract event times for SBT delivery (pass) and EHR delivery (within 2 mins)\n",
    "    sbt_d_time =(\n",
    "    final_hosp[\n",
    "        (final_hosp['sbt_delivery_pass_fail'] == 1) &\n",
    "        (final_hosp['eligible_day'] == 1)\n",
    "    ]\n",
    "    .sort_values(['hosp_id_day_key', 'event_time'])  # ensure order\n",
    "    .groupby('hosp_id_day_key', as_index=False)\n",
    "    .first()[['hosp_id_day_key', 'event_time']]\n",
    ")\n",
    "    \n",
    "    ehr_d_time = final_hosp[\n",
    "        (final_hosp['EHR_Delivery_2mins'] == 1) & \n",
    "        (final_hosp['eligible_day'] == 1)\n",
    "    ][['hosp_id_day_key', 'event_time']].drop_duplicates()\n",
    "    \n",
    "    # Convert event_time to hour values\n",
    "    sbt_hours = sbt_d_time['event_time'].dt.hour\n",
    "    ehr_hours = ehr_d_time['event_time'].dt.hour\n",
    "\n",
    "    # Create overlay histogram plot for the current hospital\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use bins from 0 to 24 (24 bins) to capture each hour of the day\n",
    "    plt.hist(sbt_hours, bins=range(0, 25), alpha=0.5, label='SBT Delivery Time', edgecolor='black')\n",
    "    plt.hist(ehr_hours, bins=range(0, 25), alpha=0.5, label='EHR Delivery Time', edgecolor='black')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Event Time Distribution (Hourly) - Hospital {hosp}')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot for the current hospital\n",
    "    plt.savefig(f\"{directory_path}/event_time_distribution_hospital_{hosp}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Build a summary DataFrame for the current hospital:\n",
    "    # Get counts per hour for each event type\n",
    "    sbt_counts = sbt_hours.value_counts().sort_index()\n",
    "    ehr_counts = ehr_hours.value_counts().sort_index()\n",
    "    \n",
    "    # Create a DataFrame with all hours 0-23, merging the counts (fill missing with 0)\n",
    "    hours_df = pd.DataFrame({'hour': range(24)})\n",
    "    hours_df['SBT_Delivery'] = hours_df['hour'].map(sbt_counts).fillna(0).astype(int)\n",
    "    hours_df['EHR_Delivery'] = hours_df['hour'].map(ehr_counts).fillna(0).astype(int)\n",
    "    hours_df['hospital_id'] = hosp\n",
    "    \n",
    "    hospital_summary_list.append(hours_df)\n",
    "\n",
    "# Combine the summary data for all hospitals into one DataFrame\n",
    "combined_summary_df = pd.concat(hospital_summary_list, ignore_index=True)\n",
    "combined_summary_df.to_csv(f\"{directory_path}/event_time_distribution_summary.csv\", index=False)\n",
    "\n",
    "print(\"Overlay plots created and summary CSV saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ids = final_df['hospital_id'].dropna().unique()\n",
    "\n",
    "# This list will hold the summary data for each hospital\n",
    "hospital_summary_list = []\n",
    "\n",
    "for hosp in hospital_ids:\n",
    "    # Filter final_df for the current hospital\n",
    "    final_hosp = final_df[final_df['hospital_id'] == hosp]\n",
    "    \n",
    "    # Extract event times for SBT delivery (pass) and EHR delivery (within 2 mins)\n",
    "    sbt_d_time = (\n",
    "    final_hosp[\n",
    "        (final_hosp['extubated'] == 1) &\n",
    "        (final_hosp['eligible_day'] == 1)\n",
    "    ]\n",
    "    .sort_values(['hosp_id_day_key', 'event_time'])  # ensure order\n",
    "    .groupby('hosp_id_day_key', as_index=False)\n",
    "    .first()[['hosp_id_day_key', 'event_time']]\n",
    ")\n",
    "\n",
    "    \n",
    "    ehr_d_time = final_hosp[\n",
    "        (final_hosp['EHR_Delivery_2mins'] == 1) & \n",
    "        (final_hosp['eligible_day'] == 1)\n",
    "    ][['hosp_id_day_key', 'event_time']].drop_duplicates()\n",
    "    \n",
    "    # Convert event_time to hour values\n",
    "    sbt_hours = sbt_d_time['event_time'].dt.hour\n",
    "    ehr_hours = ehr_d_time['event_time'].dt.hour\n",
    "\n",
    "    # Create overlay histogram plot for the current hospital\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use bins from 0 to 24 (24 bins) to capture each hour of the day\n",
    "    plt.hist(sbt_hours, bins=range(0, 25), alpha=0.5, label='Extubated Time', edgecolor='black')\n",
    "    plt.hist(ehr_hours, bins=range(0, 25), alpha=0.5, label='EHR Delivery Time', edgecolor='black')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Event Time Distribution (Hourly) - Hospital {hosp}')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot for the current hospital\n",
    "    plt.savefig(f\"{directory_path}/event_time_distribution_hospital_{hosp}_by_ex.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Build a summary DataFrame for the current hospital:\n",
    "    # Get counts per hour for each event type\n",
    "    sbt_counts = sbt_hours.value_counts().sort_index()\n",
    "    ehr_counts = ehr_hours.value_counts().sort_index()\n",
    "    \n",
    "    # Create a DataFrame with all hours 0-23, merging the counts (fill missing with 0)\n",
    "    hours_df = pd.DataFrame({'hour': range(24)})\n",
    "    hours_df['SBT_Delivery'] = hours_df['hour'].map(sbt_counts).fillna(0).astype(int)\n",
    "    hours_df['EHR_Delivery'] = hours_df['hour'].map(ehr_counts).fillna(0).astype(int)\n",
    "    hours_df['hospital_id'] = hosp\n",
    "    \n",
    "    hospital_summary_list.append(hours_df)\n",
    "\n",
    "# Combine the summary data for all hospitals into one DataFrame\n",
    "combined_summary_df = pd.concat(hospital_summary_list, ignore_index=True)\n",
    "combined_summary_df.to_csv(f\"{directory_path}/event_time_distribution_summary_by_ex.csv\", index=False)\n",
    "\n",
    "print(\"Overlay plots created and summary CSV saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate statistics from final_df ---\n",
    "\n",
    "# By n = Days\n",
    "total_days = final_df['hosp_id_day_key'].nunique()\n",
    "eligible_days = final_df[final_df['eligible_day'] == 1]['hosp_id_day_key'].nunique()\n",
    "imv_days = final_df[final_df['device_category'] == 'imv']['hosp_id_day_key'].nunique()\n",
    "percentage = (eligible_days / imv_days) * 100 if total_days > 0 else 0\n",
    "imv_icu_days = final_df[\n",
    "    (final_df['device_category'] == 'imv') & \n",
    "    (final_df['location_category'] == 'icu')\n",
    "]['hosp_id_day_key'].nunique()\n",
    "\n",
    "# By n = Encounter\n",
    "h_total_days = final_df['hospitalization_id'].nunique()\n",
    "h_eligible_days = final_df[final_df['eligible_day'] == 1]['hospitalization_id'].nunique()\n",
    "h_percentage = (h_eligible_days / h_total_days) * 100 if h_total_days > 0 else 0\n",
    "h_imv_days = final_df[final_df['device_category'] == 'imv']['hospitalization_id'].nunique()\n",
    "h_imv_icu_days = final_df[\n",
    "    (final_df['device_category'] == 'imv') & \n",
    "    (final_df['location_category'] == 'icu')\n",
    "]['hospitalization_id'].nunique()\n",
    "\n",
    "# --- Calculate statistics from mat_df ---\n",
    "\n",
    "# Distribution of EHR_Delivery_2mins for extubated == 1 (in percentages)\n",
    "ehr_delivery_counts = (\n",
    "    mat_df[mat_df['extubated'] == 1]['EHR_Delivery_2mins']\n",
    "    .value_counts(normalize=True) * 100\n",
    ")\n",
    "\n",
    "# Distribution of sbt_delivery_pass_fail for extubated == 1 (in percentages)\n",
    "sbt_delivery_counts = (\n",
    "    mat_df[mat_df['extubated'] == 1]['sbt_delivery_pass_fail']\n",
    "    .value_counts(normalize=True) * 100\n",
    ")\n",
    "\n",
    "# --- Print the statistics ---\n",
    "\n",
    "print('By n = Days')\n",
    "print('Total number of days for eval in cohort:', total_days)\n",
    "print(f\"Eligible days: {eligible_days} / {imv_days} ({percentage:.2f}%)\")\n",
    "print('Hospital days with at least one IMV event:', imv_days)\n",
    "print('Hospital days with at least one IMV & ICU event:', imv_icu_days)\n",
    "\n",
    "print('\\nBy n = Encounter')\n",
    "print('Total number of encounters for eval in cohort:', h_total_days)\n",
    "print(f\"Eligible encounters: {h_eligible_days} / {h_total_days} ({h_percentage:.2f}%)\")\n",
    "print('Encounters with at least one IMV event:', h_imv_days)\n",
    "print('Encounters with at least one IMV & ICU event:', h_imv_icu_days)\n",
    "\n",
    "print('\\nEHR_Delivery_2mins distribution (for extubated == 1):')\n",
    "print(ehr_delivery_counts)\n",
    "\n",
    "print('\\nsbt_delivery_pass_fail distribution (for extubated == 1):')\n",
    "print(sbt_delivery_counts)\n",
    "\n",
    "# --- Create a summary DataFrame for the final_df stats ---\n",
    "\n",
    "stats_data = {\n",
    "    'Metric': [\n",
    "        'total_days', 'eligible_days', 'eligible_percentage', \n",
    "        'imv_days', 'imv_icu_days', \n",
    "        'h_total_days', 'h_eligible_days', 'h_eligible_percentage', \n",
    "        'h_imv_days', 'h_imv_icu_days'\n",
    "    ],\n",
    "    'Value': [\n",
    "        total_days, eligible_days, percentage,\n",
    "        imv_days, imv_icu_days,\n",
    "        h_total_days, h_eligible_days, h_percentage,\n",
    "        h_imv_days, h_imv_icu_days\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# Convert value counts to DataFrames and append to stats_df\n",
    "ehr_counts_df = ehr_delivery_counts.reset_index()\n",
    "ehr_counts_df.columns = ['Metric', 'Value']\n",
    "ehr_counts_df['Metric'] = 'EHR_Delivery_2mins_' + ehr_counts_df['Metric'].astype(str) + '_extubated'\n",
    "\n",
    "sbt_counts_df = sbt_delivery_counts.reset_index()\n",
    "sbt_counts_df.columns = ['Metric', 'Value']\n",
    "sbt_counts_df['Metric'] = 'sbt_delivery_pass_fail_' + sbt_counts_df['Metric'].astype(str) + '_extubated'\n",
    "\n",
    "# Combine all stats\n",
    "stats_df = pd.concat([stats_df, ehr_counts_df, sbt_counts_df], ignore_index=True)\n",
    "\n",
    "print('\\nExtended statistics DataFrame with value counts:')\n",
    "print(stats_df)\n",
    "\n",
    "# Save to CSV\n",
    "stats_df.to_csv(f'{directory_path}/stats_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_hosp[\n",
    "        (mat_hosp['EHR_Delivery_2mins'] == 0) & (mat_hosp['extubated'] == 1)\n",
    "    ]['sbt_delivery_pass_fail'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
